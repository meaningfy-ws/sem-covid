{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Word2vec model training\n",
    "#### Model training based on three datasets' text data:\n",
    "- M1: pwdb + eu_timeline  ( +  ireland_timeline )\n",
    "- M2: ds_eu_cellar\n",
    "- M3: M1+M2\n",
    "\n",
    "#### Extract NOUN and NOUN PHRASES from each text data\n",
    "#### Train the word2vec model with each dataset's textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jovyan/work/sem-covid/\")\n",
    "sys.path = list(set(sys.path))\n",
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir('/home/jovyan/work/sem-covid/')\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sem_covid.adapters.data_source import IndexTabularDataSource\n",
    "from sem_covid.services.sc_wrangling.data_cleaning import (clean_text_from_specific_characters, clean_fix_unicode\n",
    ", clean_remove_currency_symbols, clean_remove_emails, clean_remove_urls)\n",
    "\n",
    "from sem_covid.entrypoints.notebooks.topic_modeling.topic_modeling_wrangling.token_management import (filter_stop_words,\n",
    "                                                                                                      select_pos,\n",
    "                                                                                                      filter_stop_words_on_a_span_list)\n",
    "\n",
    "from sem_covid.services.data_registry import Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "PWDB_TEXTUAL_CLASS = ['title', 'background_info_description', 'content_of_measure_description',\n",
    "                      'use_of_measure_description', 'involvement_of_social_partners_description']\n",
    "\n",
    "DEFAULT_TEXTUAL_COLUMN = ['title']\n",
    "WINDOW = 5\n",
    "MIN_COUNT = 10\n",
    "VECTOR_SIZE = 300\n",
    "EPOCHS = 50\n",
    "EU_TIMELINE_TOTAL_EXAMPLES = 171\n",
    "IRELAND_TIMELINE_TOTAL_EXAMPLES = 410\n",
    "EU_CELLAR_TOTAL_EXAMPLES = 2653"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preprocessing\n",
    "- data cleanup\n",
    "- turn corpus into spacy document"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def apply_cleaning_functions(document_corpus: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    This function receives the document and leads through cleaning steps\n",
    "    Args:\n",
    "        document_corpus: dataset document corpus\n",
    "\n",
    "    Returns: clean document corpus\n",
    "    \"\"\"\n",
    "    unused_characters = [\"\\\\r\", \">\", \"\\n\", \"\\\\\", \"<\", \"''\", \"%\", \"...\", \"\\'\", '\"', \"(\", \"\\n\", \"*\", \"1)\", \"2)\", \"3)\",\n",
    "                         \"[\", \"]\", \"-\", \"_\", \"\\r\"]\n",
    "\n",
    "    new_document_corpus = document_corpus.apply(clean_text_from_specific_characters, characters=unused_characters)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_fix_unicode)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_urls)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_emails)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_currency_symbols)\n",
    "\n",
    "    return new_document_corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class LanguageModelPipeline:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_sources: List[Tuple[IndexTabularDataSource, List[str]]]):\n",
    "        \"\"\"\n",
    "\n",
    "        :param dataset_sources:\n",
    "        \"\"\"\n",
    "        self.dataset_sources = dataset_sources\n",
    "        self.documents_corpus = pd.Series()\n",
    "        self.word2vec = None\n",
    "\n",
    "    def download_datasets(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.dataset_sources = [(dataset_columns, dataset_source.fetch())\n",
    "                                for dataset_source, dataset_columns in self.dataset_sources]\n",
    "\n",
    "    def extract_textual_data(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.documents_corpus = pd.concat([dataset[columns]\n",
    "                                          .fillna(value=\"\")\n",
    "                                          .agg('. '.join, axis=1)\n",
    "                                          .reset_index(drop=True)\n",
    "                                           for columns, dataset in self.dataset_sources\n",
    "                                           ], ignore_index=True)\n",
    "\n",
    "    def clean_textual_data(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.documents_corpus = apply_cleaning_functions(self.documents_corpus)\n",
    "\n",
    "    def transform_to_spacy_doc(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.documents_corpus = self.documents_corpus.apply(nlp)\n",
    "\n",
    "    def extract_pos(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.documents_corpus = self.documents_corpus.apply(select_pos, pos=\"NOUN\")\n",
    "        self.documents_corpus = self.documents_corpus.apply(lambda x: list(map(lambda docs: docs.lemma_, x)))\n",
    "\n",
    "    def model_training(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.word2vec = Word2Vec(sentences=self.documents_corpus, window=WINDOW,\n",
    "                                 min_count=MIN_COUNT, vector_size=VECTOR_SIZE)\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.download_datasets()\n",
    "        self.extract_textual_data()\n",
    "        self.clean_textual_data()\n",
    "        self.transform_to_spacy_doc()\n",
    "        self.extract_pos()\n",
    "        self.model_training()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Nr#1 language model based on:\n",
    "- PWDB\n",
    "- eu-timeline\n",
    "- ireland-timeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_sources_config = [\n",
    "    (Dataset.PWDB, PWDB_TEXTUAL_CLASS),\n",
    "    (Dataset.EU_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN),\n",
    "    (Dataset.IRELAND_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN)\n",
    "]\n",
    "language_model_pipeline = LanguageModelPipeline(dataset_sources=dataset_sources_config)\n",
    "language_model_pipeline.execute()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Nr#2 language model based on:\n",
    "- eu-cellar\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_sources_config = [\n",
    "    (Dataset.EU_CELLAR, DEFAULT_TEXTUAL_COLUMN)\n",
    "]\n",
    "language_model_pipeline = LanguageModelPipeline(dataset_sources=dataset_sources_config)\n",
    "language_model_pipeline.execute()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Nr#3 language model based on:\n",
    "- PWDB\n",
    "- eu-timeline\n",
    "- ireland-timeline\n",
    "- eu-cellar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_sources_config = [\n",
    "    (Dataset.PWDB, PWDB_TEXTUAL_CLASS),\n",
    "    (Dataset.EU_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN),\n",
    "    (Dataset.IRELAND_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN),\n",
    "    (Dataset.EU_CELLAR, DEFAULT_TEXTUAL_COLUMN)\n",
    "]\n",
    "language_model_pipeline = LanguageModelPipeline(dataset_sources=dataset_sources_config)\n",
    "language_model_pipeline.execute()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filter document tokens and select only NOUN and NOUN PHRASES"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "pwdb_noun_corpus = pwdb_spacy_corpus.apply(select_pos, pos=\"NOUN\")\n",
    "pwdb_noun_corpus = pwdb_noun_corpus.apply(lambda x: list(map(lambda docs: docs.lemma_, x)))\n",
    "\n",
    "eu_timeline_noun_corpus = eu_timeline_spacy_corpus.apply(select_pos, pos=\"NOUN\")\n",
    "eu_timeline_noun_corpus = eu_timeline_noun_corpus.apply(lambda x: list(map(lambda docs: docs.lemma_, x)))\n",
    "\n",
    "ireland_timeline_noun_corpus = ireland_timeline_spacy_corpus.apply(select_pos, pos=\"NOUN\")\n",
    "ireland_timeline_noun_corpus = ireland_timeline_noun_corpus.apply(lambda x: list(map(lambda docs: docs.lemma_, x)))\n",
    "\n",
    "eu_cellar_noun_corpus = eu_cellar_spacy_corpus.apply(select_pos, pos=\"NOUN\")\n",
    "eu_cellar_noun_corpus = eu_cellar_noun_corpus.apply(lambda x: list(map(lambda docs: docs.lemma_, x)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "pwdb_noun_phrase_corpus = pwdb_spacy_corpus.apply(lambda x: x.noun_chunks)\n",
    "pwdb_noun_phrase_corpus = pwdb_noun_phrase_corpus.apply(filter_stop_words_on_a_span_list)\n",
    "\n",
    "eu_timeline_noun_phrase_corpus = eu_timeline_spacy_corpus.apply(lambda x: x.noun_chunks)\n",
    "eu_timeline_noun_phrase_corpus = eu_timeline_noun_phrase_corpus.apply(filter_stop_words_on_a_span_list)\n",
    "\n",
    "ireland_timeline_noun_phrase_corpus = ireland_timeline_spacy_corpus.apply(lambda x: x.noun_chunks)\n",
    "ireland_timeline_noun_phrase_corpus = ireland_timeline_noun_phrase_corpus.apply(filter_stop_words_on_a_span_list)\n",
    "\n",
    "eu_cellar_noun_phrase_corpus = eu_cellar_spacy_corpus.apply(lambda x: x.noun_chunks)\n",
    "eu_cellar_noun_phrase_corpus = eu_cellar_noun_phrase_corpus.apply(filter_stop_words_on_a_span_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Word2vec model based on extracted NOUNS and NOUN PHRASES"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "(23096, 72200)"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW = 5\n",
    "MIN_COUNT = 10\n",
    "VECTOR_SIZE = 300\n",
    "EPOCHS = 50\n",
    "EU_TIMELINE_TOTAL_EXAMPLES = 171\n",
    "IRELAND_TIMELINE_TOTAL_EXAMPLES = 410\n",
    "EU_CELLAR_TOTAL_EXAMPLES = 2653\n",
    "\n",
    "m1_noun_word2vec = Word2Vec(sentences=pwdb_noun_corpus, window=WINDOW,\n",
    "                            min_count=MIN_COUNT, vector_size=VECTOR_SIZE)\n",
    "m1_noun_word2vec.train(eu_timeline_noun_corpus, total_examples=EU_TIMELINE_TOTAL_EXAMPLES, epochs=EPOCHS)\n",
    "m1_noun_word2vec.train(ireland_timeline_noun_corpus, total_examples=IRELAND_TIMELINE_TOTAL_EXAMPLES, epochs=EPOCHS)\n",
    "\n",
    "m1_noun_phrases_word2vec = Word2Vec(sentences=pwdb_noun_phrase_corpus, window=WINDOW,\n",
    "                                    min_count=MIN_COUNT, vector_size=VECTOR_SIZE)\n",
    "m1_noun_phrases_word2vec.train(eu_timeline_noun_phrase_corpus,\n",
    "                               total_examples=EU_TIMELINE_TOTAL_EXAMPLES, epochs=EPOCHS)\n",
    "m1_noun_phrases_word2vec.train(ireland_timeline_noun_phrase_corpus,\n",
    "                               total_examples=IRELAND_TIMELINE_TOTAL_EXAMPLES, epochs=EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "m2_noun_word2vec = Word2Vec(sentences=eu_cellar_noun_corpus, window=WINDOW,\n",
    "                            min_count=MIN_COUNT, vector_size=VECTOR_SIZE)\n",
    "m2_noun_phrases_word2vec = Word2Vec(sentences=eu_cellar_noun_phrase_corpus,\n",
    "                                    window=WINDOW, min_count=MIN_COUNT, vector_size=VECTOR_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "(532854, 1293000)"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3_noun_word2vec = Word2Vec(sentences=pwdb_noun_corpus, window=WINDOW,\n",
    "                            min_count=MIN_COUNT, vector_size=VECTOR_SIZE)\n",
    "m3_noun_word2vec.train(eu_timeline_noun_corpus, total_examples=EU_TIMELINE_TOTAL_EXAMPLES, epochs=EPOCHS)\n",
    "m3_noun_word2vec.train(ireland_timeline_noun_corpus, total_examples=IRELAND_TIMELINE_TOTAL_EXAMPLES, epochs=EPOCHS)\n",
    "m3_noun_word2vec.train(eu_cellar_noun_corpus, total_examples=EU_CELLAR_TOTAL_EXAMPLES, epochs=EPOCHS)\n",
    "\n",
    "m3_noun_phrases_word2vec = Word2Vec(sentences=pwdb_noun_phrase_corpus, window=WINDOW,\n",
    "                                    min_count=MIN_COUNT, vector_size=VECTOR_SIZE)\n",
    "m3_noun_phrases_word2vec.train(eu_timeline_noun_phrase_corpus, total_examples=EU_TIMELINE_TOTAL_EXAMPLES, epochs=EPOCHS)\n",
    "m3_noun_phrases_word2vec.train(ireland_timeline_noun_phrase_corpus,\n",
    "                               total_examples=IRELAND_TIMELINE_TOTAL_EXAMPLES, epochs=EPOCHS)\n",
    "m3_noun_phrases_word2vec.train(eu_cellar_noun_phrase_corpus, total_examples=EU_CELLAR_TOTAL_EXAMPLES, epochs=EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}