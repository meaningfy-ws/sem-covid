{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Word2vec model training\n",
    "#### Model training based on three datasets' text data:\n",
    "- M1: pwdb + eu_timeline  ( +  ireland_timeline )\n",
    "- M2: ds_eu_cellar\n",
    "- M3: M1+M2\n",
    "\n",
    "#### Extract NOUN and NOUN PHRASES from each text data\n",
    "#### Train the word2vec model with each dataset's textual data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jovyan/work/sem-covid/\")\n",
    "sys.path = list(set(sys.path))\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/home/jovyan/work/sem-covid/')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "from typing import List, Tuple\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 1500000\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from d3graph import d3graph\n",
    "\n",
    "\n",
    "from sem_covid.services.data_registry import Dataset\n",
    "from sem_covid.services.store_registry import store_registry\n",
    "from sem_covid.adapters.data_source import IndexTabularDataSource\n",
    "\n",
    "from sem_covid.entrypoints.notebooks.topic_modeling.topic_modeling_wrangling.token_management import select_pos\n",
    "\n",
    "from sem_covid.services.sc_wrangling.data_cleaning import (clean_text_from_specific_characters, clean_fix_unicode,\n",
    "                                                           clean_remove_currency_symbols, clean_remove_emails,\n",
    "                                                           clean_remove_urls, clean_remove_stopwords)\n",
    "\n",
    "from sem_covid.entrypoints.notebooks.language_modeling.language_model_tools.similarity_calculus import (\n",
    "    euclidean_similarity, manhattan_similarity, cosine_similarity, build_similarity_matrix)\n",
    "\n",
    "from sem_covid.entrypoints.notebooks.language_modeling.language_model_tools.document_handling_tools import(\n",
    "    document_atomization_noun_phrases, lemmatize_document)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "PWDB_TEXTUAL_CLASS = ['title', 'background_info_description', 'content_of_measure_description',\n",
    "                      'use_of_measure_description', 'involvement_of_social_partners_description']\n",
    "\n",
    "DEFAULT_TEXTUAL_COLUMN = ['title']\n",
    "WINDOW = 5\n",
    "MIN_COUNT = 1\n",
    "VECTOR_SIZE = 300\n",
    "EPOCHS = 50\n",
    "EU_TIMELINE_TOTAL_EXAMPLES = 171\n",
    "IRELAND_TIMELINE_TOTAL_EXAMPLES = 410\n",
    "EU_CELLAR_TOTAL_EXAMPLES = 2653\n",
    "\n",
    "KEY_WORDS = ['work', 'agreement', 'working', 'companies', 'workers',\n",
    "             'measures', 'temporary', 'social', 'support', 'covid19',\n",
    "             '2020', 'public', 'national', 'ireland', 'statement', '2021',\n",
    "             'announce', 'health', 'minister', 'new', 'billion', 'coronavirus',\n",
    "             'vaccine', 'eur', 'support', 'million', 'commission', 'eu']\n",
    "\n",
    "NR1_MODEL_NAME = 'model1'\n",
    "NR2_MODEL_NAME = 'model2'\n",
    "NR3_MODEL_NAME = 'model3'\n",
    "\n",
    "MODEL1_FILE_NAME = 'model1_language_model.model'\n",
    "MODEL2_FILE_NAME = 'model2_language_model.model'\n",
    "MODEL3_FILE_NAME = 'model3_language_model.model'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preprocessing\n",
    "- data cleanup\n",
    "- turn corpus into spacy document"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def add_space_between_dots_and_commas(text: str):\n",
    "    return re.sub(r'(?<=[.,])(?=[^\\s])', r' ', text)\n",
    "\n",
    "def apply_cleaning_functions(document_corpus: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    This function receives the document and leads through cleaning steps\n",
    "    Args:\n",
    "        document_corpus: dataset document corpus\n",
    "\n",
    "    Returns: clean document corpus\n",
    "    \"\"\"\n",
    "    unused_characters = [\"\\\\r\", \">\", \"\\n\", \"\\\\\", \"<\", \"''\", \"%\", \"...\", \"\\'\", '\"', \"(\", \"\\n\", \"*\", \"1)\", \"2)\", \"3)\",\n",
    "                         \"[\", \"]\", \"-\", \"_\", \"\\r\", '®', '..']\n",
    "\n",
    "    new_document_corpus = document_corpus.apply(clean_text_from_specific_characters, characters=unused_characters)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_fix_unicode)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_urls)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_emails)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_currency_symbols)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_stopwords)\n",
    "    new_document_corpus = new_document_corpus.apply(add_space_between_dots_and_commas)\n",
    "\n",
    "    return new_document_corpus\n",
    "\n",
    "\n",
    "def generate_graph(similarity_matrix: pd.DataFrame, graph: nx.Graph, root_word: str,\n",
    "                   top_words: int, threshold: np.float64 = 0.8, deep_level: int = 0,\n",
    "                   max_deep_level: int = 2, deep_map: dict = None, color_map: dict = None) -> nx.Graph:\n",
    "    \"\"\"\n",
    "        Generates d3 graph using the inserted keywords and their top words from similarity matrix\n",
    "    Args:\n",
    "        similarity_matrix: Dataframe with word similarity\n",
    "        graph: networkx graph\n",
    "        root_word: key words\n",
    "        top_words: top similar words from inserted keywords\n",
    "        threshold: minimum percentage of similarity\n",
    "        deep_level: the level of generating leaf\n",
    "        max_deep_level: the maximum number of generated leaf\n",
    "        deep_map: dictionary of the words and their level of similarity\n",
    "        color_map: the color of each level of words' similarity\n",
    "\n",
    "    Returns: a d3 graph with title and root of key word and their similarity words\n",
    "    \"\"\"\n",
    "    if root_word not in deep_map.keys():\n",
    "        deep_map[root_word] = (deep_level, color_map[deep_level])\n",
    "    elif deep_map[root_word][0] > deep_level:\n",
    "        deep_map[root_word] = (deep_level, color_map[deep_level])\n",
    "    if deep_level > max_deep_level:\n",
    "        return graph\n",
    "    new_nodes = similarity_matrix[root_word].sort_values(ascending=False)[:top_words].index.to_list()\n",
    "    new_nodes_weight = list(similarity_matrix[root_word].sort_values(ascending=False)[:top_words].values)\n",
    "    for index in range(0, len(new_nodes)):\n",
    "        if new_nodes_weight[index] >= threshold:\n",
    "            graph.add_edge(root_word, new_nodes[index])\n",
    "            generate_graph(similarity_matrix, graph, new_nodes[index], top_words, threshold, deep_level+1, max_deep_level,\n",
    "                           deep_map=deep_map, color_map=color_map)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def create_graph_for_language_model_key_words(similarity_matrix: pd.DataFrame, language_model_words: list,\n",
    "                                              model_name: str, metric_threshold: np.float64) -> d3graph:\n",
    "    \"\"\"\n",
    "    !!! This is not reusable function. It was made for a single thing !!!\n",
    "\n",
    "    It generates d3graph based on language model selected words and and the similarity\n",
    "    matrix created with those words.\n",
    "    \"\"\"\n",
    "    graph_folder_path = f'docs/word-similarity-web/{model_name}_graphs/'\n",
    "    color_map = {0: '#a70000',\n",
    "                 1: '#f0000',\n",
    "                 2: '#ff7b7b',\n",
    "                 3: '#ffbaba'}\n",
    "    for index in range(0, len(language_model_words)):\n",
    "        deep_map = {}\n",
    "        graph = generate_graph(similarity_matrix, nx.Graph(), language_model_words[index],\n",
    "                               top_words=4, threshold=metric_threshold\n",
    "                               , max_deep_level=2, deep_map=deep_map, color_map=color_map)\n",
    "        network_adjacency_matrix = pd.DataFrame(data=nx.adjacency_matrix(graph).todense(),\n",
    "                                                index=graph.nodes(), columns=graph.nodes())\n",
    "        node_color_list = [deep_map[node][0] for node in graph.nodes()]\n",
    "        d3graph(network_adjacency_matrix, savepath=graph_folder_path, savename=language_model_words[index],\n",
    "                node_color=node_color_list,\n",
    "                width=1920, height=1080, edge_width=5,\n",
    "                edge_distance=60, directed=True)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class LanguageModelPipeline:\n",
    "    \"\"\"\n",
    "        This pipeline executes the steps for word2vec language training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_sources: List[Tuple[IndexTabularDataSource, List[str]]], language_model_name: str):\n",
    "        \"\"\"\n",
    "            :param dataset_sources: represents the source of the datasets.\n",
    "        \"\"\"\n",
    "        self.dataset_sources = dataset_sources\n",
    "        self.language_model_name = language_model_name\n",
    "        self.documents_corpus = pd.Series()\n",
    "        self.word2vec = None\n",
    "\n",
    "    def download_datasets(self):\n",
    "        \"\"\"\n",
    "            In this step it will download the dataset and detect selected columns.\n",
    "            It can be downloaded as many datasets as there are in data source.\n",
    "        \"\"\"\n",
    "        self.dataset_sources = [(dataset_columns, dataset_source.fetch())\n",
    "                                for dataset_source, dataset_columns in self.dataset_sources]\n",
    "\n",
    "    def extract_textual_data(self):\n",
    "        \"\"\"\n",
    "            After downloading the datasets, the textual data will be found and and concatenated\n",
    "            with executing of several steps as well. It will fill the NaN values with empty space,\n",
    "            add a dot at the end of each concatenated column and reset the index.\n",
    "        \"\"\"\n",
    "        self.documents_corpus = pd.concat([dataset[columns]\n",
    "                                          .fillna(value=\"\")\n",
    "                                          .agg('. '.join, axis=1)\n",
    "                                          .reset_index(drop=True)\n",
    "                                           for columns, dataset in self.dataset_sources\n",
    "                                           ], ignore_index=True)\n",
    "\n",
    "    def clean_textual_data(self):\n",
    "        \"\"\"\n",
    "            The next step is data cleaning. In this step the function \"apply_cleaning_functions\"\n",
    "            applies the following actions:\n",
    "                - clean the document from specific characters\n",
    "                - delete unicode\n",
    "                - removes emails and URLs and currency symbols\n",
    "        \"\"\"\n",
    "        self.documents_corpus = apply_cleaning_functions(self.documents_corpus)\n",
    "\n",
    "    def transform_to_spacy_doc(self):\n",
    "        \"\"\"\n",
    "            When the document is clean, is going to be transform into spacy document\n",
    "        \"\"\"\n",
    "        self.documents_corpus = self.documents_corpus.apply(nlp)\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "            To extract the parts of speech, below it was defined classes for each token is necessary.\n",
    "        \"\"\"\n",
    "        self.documents_corpus = pd.concat([self.documents_corpus,\n",
    "                                           self.documents_corpus.apply(document_atomization_noun_phrases),\n",
    "                                           self.documents_corpus.apply(lemmatize_document)]\n",
    "                                           ,ignore_index=True)\n",
    "\n",
    "        self.documents_corpus = self.documents_corpus.apply(lambda x: list(map(str, x)))\n",
    "\n",
    "    def model_training(self):\n",
    "        \"\"\"\n",
    "            When the data is prepared it's stored into Word2Vec model.\n",
    "        \"\"\"\n",
    "        self.word2vec = Word2Vec(sentences=self.documents_corpus, window=WINDOW,\n",
    "                                 min_count=MIN_COUNT, vector_size=VECTOR_SIZE)\n",
    "\n",
    "    def save_language_model(self):\n",
    "        \"\"\"\n",
    "            Saves trained model in MinIO\n",
    "        \"\"\"\n",
    "        minio = store_registry.minio_object_store('mdl-language')\n",
    "        minio.put_object('word2vec/' + self.language_model_name, pickle.dumps(self.word2vec))\n",
    "\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"\n",
    "            The final step is execution, where are stored each step and it will be executed in a row\n",
    "        \"\"\"\n",
    "        self.download_datasets()\n",
    "        self.extract_textual_data()\n",
    "        self.clean_textual_data()\n",
    "        self.transform_to_spacy_doc()\n",
    "        self.extract_features()\n",
    "        self.model_training()\n",
    "        self.save_language_model()\n",
    "\n",
    "\n",
    "class LanguageModelWordsFilter:\n",
    "    def __init__(self, word2vec_model: Word2Vec, key_words: List[str], pos: List[str]) -> None:\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.key_words = key_words\n",
    "        self.pos = pos\n",
    "        self.word2vec_document = None\n",
    "        self.word2vec_document = nlp(' '.join(self.word2vec_model.wv.index_to_key))\n",
    "        self.word2vec_document = select_pos(self.word2vec_document, self.pos)\n",
    "        self._extract_pos = list(map(str, self.word2vec_document))\n",
    "\n",
    "    def extract_pos(self) -> List[str]:\n",
    "        \"\"\"\n",
    "            transforms a word2vec indexes into spacy document and selects parts of\n",
    "            speech. After that it puts into a list and converts those parts of speech\n",
    "            into strings.\n",
    "        \"\"\"\n",
    "        return self._extract_pos\n",
    "\n",
    "    def select_key_words(self) -> List[str]:\n",
    "        \"\"\"\n",
    "            Finds each word form inserted list of key words and returns a\n",
    "            list with those words if there are presented in the list of\n",
    "            extracted parts of speech.\n",
    "        \"\"\"\n",
    "        return [word for word in self.key_words if word in self._extract_pos]\n",
    "\n",
    "    def select_pos_index(self) -> List[int]:\n",
    "        \"\"\"\n",
    "            Detects the part of speech indexes and returns them into a list\n",
    "        \"\"\"\n",
    "        return [self.word2vec_model.wv.index_to_key.index(token) for token in self._extract_pos\n",
    "                if token in self.word2vec_model.wv.index_to_key]\n",
    "\n",
    "    def select_pos_embeddings(self) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "            Detects part of speech embeddings from their indexes\n",
    "        \"\"\"\n",
    "        selected_pos_index = self.select_pos_index()\n",
    "        return [self.word2vec_model.wv.vectors[index] for index in selected_pos_index]\n",
    "\n",
    "\n",
    "def train_language_model(dataset_sources_config: List[tuple], language_model_name: str):\n",
    "    \"\"\"\n",
    "        1. creates word2vec model with LanguageModelPipeline\n",
    "        2. filters nouns and adjectives\n",
    "     Args:\n",
    "        dataset_sources_config: the dataset and his textual data columns\n",
    "        language_model_name: the name of the language model, to be saved in MinIO\n",
    "    Returns: Amount of filtered words\n",
    "    \"\"\"\n",
    "    model_language_model_pipeline = LanguageModelPipeline(dataset_sources=dataset_sources_config,\n",
    "                                                          language_model_name=language_model_name)\n",
    "    model_language_model_pipeline.execute()\n",
    "\n",
    "    return LanguageModelWordsFilter(model_language_model_pipeline.word2vec, KEY_WORDS, pos=['NOUN', 'ADJ'])\n",
    "\n",
    "\n",
    "def train_similarity_matrices(model_name: str, language_model: train_language_model):\n",
    "    \"\"\"\n",
    "        Generates similarity matrix using extracts pos and pos embeddings\n",
    "    Args:\n",
    "        model_name: the name of the model that will be saved on the server\n",
    "        language_model: trained word2vec model\n",
    "    \"\"\"\n",
    "    similarity_functions = [cosine_similarity, euclidean_similarity, manhattan_similarity]\n",
    "    for index in range(len(similarity_functions)):\n",
    "        print('Start computing similarity matrix.')\n",
    "        model_similarity_matrix = build_similarity_matrix(language_model.select_pos_embeddings(),\n",
    "                                                          language_model.extract_pos(),\n",
    "                                                          metric=similarity_functions[index])\n",
    "        print('Finish computing similarity matrix.')\n",
    "        print('Save similarity matrix.')\n",
    "        store_registry.minio_object_store('semantic-similarity-matrices').put_object(\n",
    "            f'{model_name}_{similarity_functions[index].__name__}_matrix.json',\n",
    "            model_similarity_matrix.to_json(orient='columns'))\n",
    "        print('Create d3Graphs')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Nr#1 language model based on:\n",
    "- PWDB\n",
    "- eu-timeline\n",
    "- ireland-timeline\n",
    "\n",
    "\n",
    "## Experiment Nr#2 language model based on:\n",
    "- eu-cellar\n",
    "\n",
    "## Experiment Nr#3 language model based on:\n",
    "- PWDB\n",
    "- eu-timeline\n",
    "- ireland-timeline\n",
    "- eu-cellar\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 1 µs, total: 9 µs\n",
      "Wall time: 12.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model1_dataset_sources_config = [\n",
    "    (Dataset.PWDB, PWDB_TEXTUAL_CLASS),\n",
    "    (Dataset.EU_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN),\n",
    "    (Dataset.IRELAND_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN)\n",
    "]\n",
    "\n",
    "model2_dataset_sources_config = [\n",
    "    (Dataset.EU_CELLAR_ENRICHED, DEFAULT_TEXTUAL_COLUMN),\n",
    "]\n",
    "\n",
    "model3_dataset_sources_config = [\n",
    "    (Dataset.PWDB, PWDB_TEXTUAL_CLASS),\n",
    "    (Dataset.EU_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN),\n",
    "    (Dataset.EU_CELLAR_ENRICHED, DEFAULT_TEXTUAL_COLUMN),\n",
    "    (Dataset.IRELAND_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train language model and execute similarity matrices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1288 of 1288) |####################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (171 of 171) |######################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (410 of 410) |######################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 9s, sys: 1.4 s, total: 3min 10s\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# !!! Below three cells will be executing a good period of time. If for any reason you run it,\n",
    "# take your time, go and make some food, go for a walk maybe. !!!\n",
    "\n",
    "# execution time: 1h 23m\n",
    "model1_word2vec = train_language_model(model1_dataset_sources_config, MODEL1_FILE_NAME)\n",
    "model1_similarity_matrix = train_similarity_matrices(NR1_MODEL_NAME, model1_word2vec)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start computing similarity matrix.\n",
      "Finish computing similarity matrix.\n",
      "Save similarity matrix.\n",
      "Create d3Graphs\n",
      "CPU times: user 3min 20s, sys: 787 ms, total: 3min 21s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# execution time: 3m\n",
    "# model2 = execute_model_steps(NR2_MODEL_NAME, model2_dataset_sources_config)\n",
    "model2_word2vec = train_language_model(model2_dataset_sources_config, MODEL2_FILE_NAME)\n",
    "model2_similarity_matrix = train_similarity_matrices(NR2_MODEL_NAME, model2_word2vec)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "# execution time: 1h 33m\n",
    "model3_word2vec = train_language_model(model3_dataset_sources_config, MODEL3_FILE_NAME)\n",
    "model3_similarity_matrix = train_similarity_matrices(NR3_MODEL_NAME, model3_word2vec)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate D3 Graphs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model1_cosine_matrix = store_registry\\\n",
    "    .minio_object_store('semantic-similarity-matrices')\\\n",
    "    .get_object('model1_cosine_similarity_matrix.json')\n",
    "\n",
    "\n",
    "model1_graphs = create_graph_for_language_model_key_words(pd.read_json(model1_cosine_matrix),\n",
    "                                                          model1_word2vec.select_key_words(),\n",
    "                                                          model_name=NR2_MODEL_NAME, metric_threshold=0.4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model2_cosine_matrix = store_registry\\\n",
    "    .minio_object_store('semantic-similarity-matrices')\\\n",
    "    .get_object('model2_cosine_similarity_matrix.json')\n",
    "\n",
    "\n",
    "model2_graphs = create_graph_for_language_model_key_words(pd.read_json(model2_cosine_matrix),\n",
    "                                                          model2_word2vec.select_key_words(),\n",
    "                                                          model_name=NR3_MODEL_NAME, metric_threshold=0.4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model3_cosine_matrix = store_registry\\\n",
    "    .minio_object_store('semantic-similarity-matrices')\\\n",
    "    .get_object('model3_cosine_similarity_matrix.json')\n",
    "\n",
    "\n",
    "model3_graphs = create_graph_for_language_model_key_words(pd.read_json(model3_cosine_matrix),\n",
    "                                                          model3_word2vec.select_key_words(),\n",
    "                                                          model_name=NR3_MODEL_NAME, metric_threshold=0.4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}