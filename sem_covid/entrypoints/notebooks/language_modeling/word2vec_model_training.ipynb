{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Word2vec model training\n",
    "#### Model training based on three datasets' text data:\n",
    "- M1: pwdb + eu_timeline  ( +  ireland_timeline )\n",
    "- M2: ds_eu_cellar\n",
    "- M3: M1+M2\n",
    "\n",
    "#### Extract NOUN and NOUN PHRASES from each text data\n",
    "#### Train the word2vec model with each dataset's textual data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/jovyan/work/sem-covid/\")\n",
    "sys.path = list(set(sys.path))\n",
    "\n",
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir('/home/jovyan/work/sem-covid/')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from typing import List, Tuple\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import spacy\n",
    "import enchant\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from spacy.tokens.doc import Doc\n",
    "from gensim.models import Word2Vec\n",
    "# from gensim.parsing.preprocessing import remove_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from sem_covid.services.store_registry import store_registry\n",
    "from sem_covid.adapters.embedding_models import BasicSentenceSplitterModel\n",
    "from sem_covid.services.language_model_pipelines.language_model_execution_steps import LanguageModelExecutionSteps\n",
    "from sem_covid.services.language_model_pipelines.language_model_pipeline import LanguageModelPipeline\n",
    "from sem_covid.entrypoints.notebooks.language_modeling.language_model_tools.document_handling_tools import (\n",
    "    document_atomization_noun_phrases, clean_text)\n",
    "from sem_covid.entrypoints.notebooks.language_modeling.language_model_tools.similarity_calculus import (\n",
    "    build_similarity_matrix)\n",
    "from sem_covid.entrypoints.notebooks.language_modeling.language_model_tools.graph_handling import (\n",
    "    create_graph_for_language_model_key_words)\n",
    "\n",
    "\n",
    "\n",
    "# en_words = set(words.words())\n",
    "# d = enchant.Dict(\"en_US\")\n",
    "# nltk.download('words')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "nlp.max_length = 5000000\n",
    "WINDOW = 5\n",
    "MIN_COUNT = 1\n",
    "VECTOR_SIZE = 300\n",
    "LANGUAGE_MODEL_MINIO_FOLDER = 'word2vec/'\n",
    "LANGUAGE_MODEL_BUCKET_NAME = 'mdl-language'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# economic\n",
    "## Define constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "DEFAULT_TEXTUAL_CLASS = ['Title', 'Content']\n",
    "\n",
    "PWDB_TEXTUAL_CLASS = ['title', 'background_info_description', 'content_of_measure_description',\n",
    "                      'country', 'category', 'subcategory', 'target_groups']\n",
    "\n",
    "EU_CELLAR_TEXTUAL_CLASS = ['title', 'content', 'eurovoc_concept_labels', 'subject_matter_labels',\n",
    "                           'directory_codes_labels']\n",
    "\n",
    "IRELAND_ACTION_TIMELINE_CLASS = ['title', 'content', 'keyword']\n",
    "\n",
    "EU_ACTION_TIMELINE_CLASS = ['abstract', 'title', 'topics', 'detail_content']\n",
    "\n",
    "KEY_WORDS_FOR_ALL_MODELS = [\"eu\", \"national\", \"work\", \"aid\", \"coronavirus\", \"covid19\", \"measures\",\n",
    "                            \"vaccine\", \"minister\", \"government\", \"organisations\",\n",
    "                            \"agreement\", \"unemployment\", \"insurance\", \"reorientation\", \"economy\",\n",
    "                            \"economic\", \"innovation\", \"research\", \"development\", \"risk\", \"transport\"]\n",
    "\n",
    "COUNTRIES = ['austria', 'belgium', 'bulgaria', 'croatia', 'cyprus', 'czechia', 'denmark', 'estonia',\n",
    "             'european_union', 'finland', 'france', 'germany', 'greece', 'hungary', 'ireland', 'italy',\n",
    "             'latvia', 'lithuania', 'luxembourg', 'malta', 'netherlands', 'norway', 'poland', 'portugal',\n",
    "             'romania', 'slovakia', 'slovenia', 'spain', 'sweden', 'united_kingdom']\n",
    "\n",
    "CATEGORY = ['retention', 'workplace', 'labour', 'recovery', 'adaptation',\n",
    "            'protection', 'essential', 'business_continuity',\n",
    "            'services', 'social', 'market']\n",
    "\n",
    "SUBCATEGORY = ['safety', 'arrangements', 'health', 'spending', 'working', 'support', 'occupational',\n",
    "               'stimulus_packages', 'access', 'time', 'finance', 'remote', 'flexibility',\n",
    "               'essential_services', 'remuneration']\n",
    "\n",
    "TARGET_GROUPS_L1 = ['businesses', 'workers', 'citizens']\n",
    "\n",
    "TARGET_GROUPS_L2 = ['company', 'older', 'people', 'female', 'aged', 'corporations',\n",
    "                    'single', 'person', 'forms', 'smes', 'ups', 'single_parents',\n",
    "                    'citizens', 'professions', 'parents', 'groups', 'youth',\n",
    "                    'sector', 'women', 'unemployed', 'care', 'facilities', 'standard',\n",
    "                    'specific', 'contractors', 'children', 'border', 'refugees',\n",
    "                    'minors', 'platform', 'employment', 'seasonal', 'disabled', 'migrants',\n",
    "                    'risk_group', 'commuters']\n",
    "\n",
    "FUNDING = ['companies', 'national_funds', 'employer', 'funds', 'european_funds', 'no_special_funding_required',\n",
    "           'regional_funds', 'local_funds', 'employers_organization', 'employees']\n",
    "\n",
    "WORDS_PACK1 = {'category': CATEGORY,\n",
    "               'subcategory': SUBCATEGORY,\n",
    "               'countries': COUNTRIES,\n",
    "               'target_groups_l1': TARGET_GROUPS_L1,\n",
    "               'target_groups_l2': TARGET_GROUPS_L2,\n",
    "               'funding': FUNDING}\n",
    "\n",
    "WORDS_PACK2 = {'keywords': KEY_WORDS_FOR_ALL_MODELS}\n",
    "\n",
    "MODEL_WORDS_PACKS = (WORDS_PACK1, WORDS_PACK1, WORDS_PACK2)\n",
    "\n",
    "MODEL_NAMES = ('model1', 'model2', 'model3')\n",
    "\n",
    "FILE_NAMES = ('model1_language_model.model',\n",
    "              'model2_language_model.model',\n",
    "              'model3_language_model.model'\n",
    "              )\n",
    "\n",
    "SIMILARITY_MATRIX_BUCKET_NAME = 'semantic-similarity-matrices'\n",
    "\n",
    "COSINE_SIMILARITY_MATRICES = ('model1_cosine_matrix.pkl',\n",
    "                              'model2_cosine_matrix.pkl',\n",
    "                              'model3_cosine_matrix.pkl'\n",
    "                              )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preprocessing\n",
    "- data cleanup\n",
    "- turn corpus into spacy document\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Nr#1 language model based on:\n",
    "- PWDB\n",
    "- eu-timeline\n",
    "- ireland-timeline\n",
    "\n",
    "## Experiment Nr#2 language model based on:\n",
    "- eu-cellar\n",
    "\n",
    "## Experiment Nr#3 language model based on:\n",
    "- PWDB\n",
    "- eu-timeline\n",
    "- ireland-timeline\n",
    "- eu-cellar\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (4126 of 4126) |####################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "ds_unified = store_registry.es_index_store().get_dataframe('ds_unified_datasets')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model1_df = ds_unified.query('Document_source != \"eu_cellar\"')\n",
    "model2_df = ds_unified.query('Document_source == \"eu_cellar\"')\n",
    "model3_df = ds_unified\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "MODEL_DATASET_SOURCES_CONFIGS = [\n",
    "    (model1_df, DEFAULT_TEXTUAL_CLASS),\n",
    "    (model2_df, DEFAULT_TEXTUAL_CLASS),\n",
    "    (model3_df, DEFAULT_TEXTUAL_CLASS),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def clean_text(text: str) -> str:\n",
    "#     tmp_word_list = re.sub(' +', ' ', re.sub(r'[^a-zA-Z_ ]', ' ', text)).lower().split(' ')\n",
    "#     result_words = [word\n",
    "#                     for word in tmp_word_list\n",
    "#                     if len(word) > 3 and (word in en_words or d.check(word))]\n",
    "#     if len(result_words) > 3:\n",
    "#         return ' '.join(result_words)\n",
    "#     else:\n",
    "#         return ''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def apply_cleaning_functions(document_corpus: list) -> list:\n",
    "#     \"\"\"\n",
    "#     This function receives the document and leads through cleaning steps\n",
    "#     Args:\n",
    "#         document_corpus: dataset document corpus\n",
    "#\n",
    "#     Returns: clean document corpus\n",
    "#     \"\"\"\n",
    "#     splitter = BasicSentenceSplitterModel()\n",
    "#     textual_data = '. '.join(document_corpus)\n",
    "#     splitted_text = splitter.split(textual_data)\n",
    "#     splitted_long_text = [sent for sent in splitted_text if len(sent) > 10]\n",
    "#     cleaned_text = list(set([sent for sent in list(map(clean_text, splitted_long_text)) if sent]))\n",
    "#     return cleaned_text\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def document_atomization_noun_phrases(document: Doc) -> str:\n",
    "#     \"\"\"\n",
    "#         Detects each noun phrase from inserted spacy document and transforms it\n",
    "#         into integrate single token\n",
    "#         :document: spacy document\n",
    "#         :return: The same document, but with atomized noun phrases\n",
    "#     \"\"\"\n",
    "#     sentence = ' '.join([word.lemma_ for word in document])\n",
    "#     for noun_phrase in document.noun_chunks:\n",
    "#         noun_phrase_lemma = [x.lemma_ for x in noun_phrase]\n",
    "#         sequence = \" \".join(\n",
    "#             [token for token in noun_phrase_lemma if token != \"\" and token != \" \"])\n",
    "#         sequence_without_stopwords = remove_stopwords(sequence)\n",
    "#         sentence = sentence.replace(sequence, sequence_without_stopwords.replace(' ', '_'))\n",
    "#\n",
    "#     return remove_stopwords(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "# class LanguageModelPipeline:\n",
    "#     \"\"\"\n",
    "#         This pipeline executes the steps for word2vec language training.\n",
    "#     \"\"\"\n",
    "#\n",
    "#     def __init__(self, dataset_source: pd.DataFrame, textual_columns: List[str], language_model_name: str):\n",
    "#         \"\"\"\n",
    "#             :param dataset_sources: represents the source of the datasets.\n",
    "#         \"\"\"\n",
    "#         self.dataset_source = dataset_source\n",
    "#         self.textual_columns = textual_columns\n",
    "#         self.language_model_name = language_model_name\n",
    "#         self.documents_corpus = pd.Series()\n",
    "#         self.word2vec = None\n",
    "#         self.steps = [self.extract_textual_data,\n",
    "#                       self.clean_textual_data,\n",
    "#                       self.transform_to_spacy_doc,\n",
    "#                       self.extract_features,\n",
    "#                       self.model_training,\n",
    "#                       self.save_language_model\n",
    "#                       ]\n",
    "#\n",
    "#     def extract_textual_data(self):\n",
    "#         \"\"\"\n",
    "#             After downloading the datasets, the textual data will be found and and concatenated\n",
    "#             with executing of several steps as well. It will fill the NaN values with empty space,\n",
    "#             add a dot at the end of each concatenated column and reset the index.\n",
    "#         \"\"\"\n",
    "#         self.documents_corpus = (self.dataset_source[self.textual_columns]\n",
    "#                                  .fillna(value=\"\")\n",
    "#                                  .agg('. '.join, axis=1).values\n",
    "#                                  )\n",
    "#\n",
    "#     def clean_textual_data(self):\n",
    "#         \"\"\"\n",
    "#             The next step is data cleaning. In this step the function \"apply_cleaning_functions\"\n",
    "#             applies the following actions:\n",
    "#                 - clean the document from specific characters\n",
    "#                 - delete unicode\n",
    "#                 - removes emails and URLs and currency symbols\n",
    "#         \"\"\"\n",
    "#         self.documents_corpus = apply_cleaning_functions(self.documents_corpus)\n",
    "#\n",
    "#     def transform_to_spacy_doc(self):\n",
    "#         \"\"\"\n",
    "#             When the document is clean, is going to be transform into spacy document\n",
    "#         \"\"\"\n",
    "#         self.documents_corpus = list(nlp.pipe(self.documents_corpus))\n",
    "#\n",
    "#     def extract_features(self):\n",
    "#         \"\"\"\n",
    "#             To extract the parts of speech, below it was defined classes for each token is necessary.\n",
    "#         \"\"\"\n",
    "#         tmp_documents_corpus = self.documents_corpus.copy()\n",
    "#         tmp_documents_corpus = [\n",
    "#             ' '.join([word.lemma_ for word in document])\n",
    "#             for document in tmp_documents_corpus\n",
    "#         ]\n",
    "#         self.documents_corpus = list(map(document_atomization_noun_phrases, self.documents_corpus))\n",
    "#         self.documents_corpus = self.documents_corpus + tmp_documents_corpus\n",
    "#\n",
    "#     def model_training(self):\n",
    "#         \"\"\"\n",
    "#             When the data is prepared it's stored into Word2Vec model.\n",
    "#         \"\"\"\n",
    "#         self.word2vec = Word2Vec(sentences=list(map(lambda x: x.split(' '), self.documents_corpus)),\n",
    "#                                  window=WINDOW,\n",
    "#                                  min_count=MIN_COUNT, vector_size=VECTOR_SIZE)\n",
    "#\n",
    "#     def save_language_model(self):\n",
    "#         \"\"\"\n",
    "#             Saves trained model in MinIO\n",
    "#         \"\"\"\n",
    "#         minio = store_registry.minio_object_store(LANGUAGE_MODEL_BUCKET_NAME)\n",
    "#         minio.put_object(LANGUAGE_MODEL_MINIO_FOLDER + self.language_model_name, pickle.dumps(self.word2vec))\n",
    "#\n",
    "#     def execute(self):\n",
    "#         \"\"\"\n",
    "#             The final step is execution, where are stored each step and it will be executed in a row\n",
    "#         \"\"\"\n",
    "#         for step in self.steps:\n",
    "#             start_time = time.time()\n",
    "#             print(f'Start: {step.__name__}')\n",
    "#             step()\n",
    "#             end_time = time.time()\n",
    "#             print(f'Finish: {step.__name__}')\n",
    "#             print(f'Time elapsed: {round(end_time - start_time, 4)} seconds')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: extract_textual_data\n",
      "Finish: extract_textual_data\n",
      "Time elapsed: 0.0321 seconds\n",
      "Start: clean_textual_data\n",
      "Finish: clean_textual_data\n",
      "Time elapsed: 9.9328 seconds\n",
      "Start: transform_to_spacy_doc\n",
      "Finish: transform_to_spacy_doc\n",
      "Time elapsed: 184.0714 seconds\n",
      "Start: extract_features\n",
      "Finish: extract_features\n",
      "Time elapsed: 6.3946 seconds\n",
      "Start: model_training\n",
      "Finish: model_training\n",
      "Time elapsed: 23.3179 seconds\n",
      "Start: save_language_model\n",
      "Finish: save_language_model\n",
      "Time elapsed: 2.9704 seconds\n"
     ]
    }
   ],
   "source": [
    "lang_model_pipeline = LanguageModelPipeline(dataset_source=model2_df,\n",
    "                                            textual_columns=DEFAULT_TEXTUAL_CLASS,\n",
    "                                            language_model_name=MODEL_NAMES[1])\n",
    "\n",
    "lang_model_pipeline.execute()\n",
    "word2vec = lang_model_pipeline.word2vec\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "def get_filtered_words(corpus):\n",
    "    tfidf_vec = TfidfVectorizer(use_idf=True)\n",
    "    transformed = tfidf_vec.fit_transform(raw_documents=corpus)\n",
    "    words = list(tfidf_vec.vocabulary_.items())\n",
    "    idf = tfidf_vec.idf_\n",
    "    weighted_words = [(pair[0], idf[pair[1]]) for pair in words]\n",
    "    tmp_df = pd.DataFrame(weighted_words, columns=['word', 'idf'])\n",
    "    filter_limit = tmp_df.groupby(by='idf').agg(['count']).tail(2).index[0]\n",
    "    words = tmp_df[tmp_df.idf < filter_limit].word.values.tolist()\n",
    "    nlp_documents = list(nlp.pipe(words))\n",
    "    filtered_words = [str(word) for document in nlp_documents\n",
    "                      for word in document\n",
    "                      if word.pos_ in ['NOUN', 'ADJ']]\n",
    "    return filtered_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "filtered_words = get_filtered_words(lang_model_pipeline.documents_corpus)\n",
    "\n",
    "vectors = [word2vec.wv.vectors[word2vec.wv.key_to_index[word]] for word in filtered_words]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarity_functions = ['cosine', 'euclidean']\n",
    "for index in range(len(similarity_functions)):\n",
    "    print('Start computing similarity matrix.')\n",
    "    model_similarity_matrix = build_similarity_matrix(\n",
    "        np.array(vectors),\n",
    "        filtered_words,\n",
    "        metric=similarity_functions[index])\n",
    "    print('Finish computing similarity matrix.')\n",
    "    print('Save similarity matrix.')\n",
    "    store_registry.minio_feature_store('semantic-similarity-matrices').put_features(\n",
    "        features_name=f'model2_{similarity_functions[index]}_matrix.pkl',\n",
    "        content=model_similarity_matrix\n",
    "    )\n",
    "    del model_similarity_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate D3 Graphs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cosine similarity graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def plot_graphs(pipeline: LanguageModelExecutionSteps, model_name: str, model_file_name: str,\n",
    "                threshold: np.float64, word_graph_configs: dict, normalize_func) -> None:\n",
    "    \"\"\"\n",
    "        steps of generating d3 graph, calling the similarity matrix from minio and normalizing it.\n",
    "    Args:\n",
    "        pipeline: Pipeline of language model execution stems\n",
    "        model_name: the name of the model\n",
    "        model_file_name: word2vec file name from MinIO\n",
    "        threshold: the minimum of similarity number\n",
    "        word_graph_configs: dictionary of key words\n",
    "        normalize_func: function of similarity normalization\n",
    "    \"\"\"\n",
    "    model_cosine_matrix = store_registry.minio_feature_store(SIMILARITY_MATRIX_BUCKET_NAME).get_features(\n",
    "        model_file_name)\n",
    "    model_cosine_matrix = model_cosine_matrix.applymap(normalize_func)\n",
    "    for key in word_graph_configs.keys():\n",
    "        create_graph_for_language_model_key_words(model_cosine_matrix,\n",
    "                                                  pipeline.filter_language_model_words().select_key_words(\n",
    "                                                      key_words=word_graph_configs[key]),\n",
    "                                                  model_name=model_name,\n",
    "                                                  metric_threshold=threshold, column_name=key)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def execute_language_model_pipeline(model_file_name: str,\n",
    "                                    model_name: str,\n",
    "                                    model_dataset_sources_config: List[tuple],\n",
    "                                    #model_words_pack: dict,\n",
    "                                    #cosine_similarity_matrix: str\n",
    "                                    ):\n",
    "    model_execution_steps = LanguageModelExecutionSteps(language_model_file_name=model_file_name,\n",
    "                                                        model_name=model_name)\n",
    "    model_execution_steps.train_language_model(model_dataset_sources_config)\n",
    "    model_execution_steps.train_similarity_matrices()\n",
    "    # plot_graphs(pipeline=model_execution_steps,\n",
    "    #             model_name=model_name,\n",
    "    #             model_file_name=cosine_similarity_matrix,\n",
    "    #             threshold=0.6,\n",
    "    #             word_graph_configs=model_words_pack,\n",
    "    #             normalize_func=lambda x: 1 - x)\n",
    "    del model_execution_steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}