{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Word2vec model training\n",
    "#### Model training based on three datasets' text data:\n",
    "- M1: pwdb + eu_timeline  ( +  ireland_timeline )\n",
    "- M2: ds_eu_cellar\n",
    "- M3: M1+M2\n",
    "\n",
    "#### Extract NOUN and NOUN PHRASES from each text data\n",
    "#### Train the word2vec model with each dataset's textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"/home/jovyan/work/sem-covid/\")\n",
    "sys.path = list(set(sys.path))\n",
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir('/home/jovyan/work/sem-covid/')\n",
    "\n",
    "import warnings\n",
    "\n",
    "from spacy.tokens.doc import Doc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sem_covid.adapters.data_source import IndexTabularDataSource\n",
    "from sem_covid.services.sc_wrangling.data_cleaning import (clean_text_from_specific_characters, clean_fix_unicode,\n",
    "                                                           clean_remove_currency_symbols, clean_remove_emails,\n",
    "                                                           clean_remove_urls)\n",
    "\n",
    "from sem_covid.entrypoints.notebooks.topic_modeling.topic_modeling_wrangling.token_management import (select_pos,\n",
    "                                                                                                      filter_stop_words_on_a_span_list)\n",
    "\n",
    "from sem_covid.services.data_registry import Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "PWDB_TEXTUAL_CLASS = ['title', 'background_info_description', 'content_of_measure_description',\n",
    "                      'use_of_measure_description', 'involvement_of_social_partners_description']\n",
    "\n",
    "DEFAULT_TEXTUAL_COLUMN = ['title']\n",
    "WINDOW = 5\n",
    "MIN_COUNT = 1\n",
    "VECTOR_SIZE = 300\n",
    "EPOCHS = 50\n",
    "EU_TIMELINE_TOTAL_EXAMPLES = 171\n",
    "IRELAND_TIMELINE_TOTAL_EXAMPLES = 410\n",
    "EU_CELLAR_TOTAL_EXAMPLES = 2653"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preprocessing\n",
    "- data cleanup\n",
    "- turn corpus into spacy document"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def apply_cleaning_functions(document_corpus: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    This function receives the document and leads through cleaning steps\n",
    "    Args:\n",
    "        document_corpus: dataset document corpus\n",
    "\n",
    "    Returns: clean document corpus\n",
    "    \"\"\"\n",
    "    unused_characters = [\"\\\\r\", \">\", \"\\n\", \"\\\\\", \"<\", \"''\", \"%\", \"...\", \"\\'\", '\"', \"(\", \"\\n\", \"*\", \"1)\", \"2)\", \"3)\",\n",
    "                         \"[\", \"]\", \"-\", \"_\", \"\\r\"]\n",
    "\n",
    "    new_document_corpus = document_corpus.apply(clean_text_from_specific_characters, characters=unused_characters)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_fix_unicode)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_urls)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_emails)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_currency_symbols)\n",
    "\n",
    "    return new_document_corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "class LanguageModelPipeline:\n",
    "    \"\"\"\n",
    "        This pipeline executes the steps for word2vec language training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_sources: List[Tuple[IndexTabularDataSource, List[str]]]):\n",
    "        \"\"\"\n",
    "            :param dataset_sources: represents the source of the datasets.\n",
    "        \"\"\"\n",
    "        self.dataset_sources = dataset_sources\n",
    "        self.documents_corpus = pd.Series()\n",
    "        self.word2vec = None\n",
    "\n",
    "    def download_datasets(self):\n",
    "        \"\"\"\n",
    "            In this step it will download the dataset and detect selected columns.\n",
    "            It can be downloaded as many datasets as there are in data source.\n",
    "        \"\"\"\n",
    "        self.dataset_sources = [(dataset_columns, dataset_source.fetch())\n",
    "                                for dataset_source, dataset_columns in self.dataset_sources]\n",
    "\n",
    "    def extract_textual_data(self):\n",
    "        \"\"\"\n",
    "            After downloading the datasets, the textual data will be found and and concatenated\n",
    "            with executing of several steps as well. It will fill the NaN values with empty space,\n",
    "            add a dot at the end of each concatenated column and reset the index.\n",
    "        \"\"\"\n",
    "        self.documents_corpus = pd.concat([dataset[columns]\n",
    "                                          .fillna(value=\"\")\n",
    "                                          .agg('. '.join, axis=1)\n",
    "                                          .reset_index(drop=True)\n",
    "                                           for columns, dataset in self.dataset_sources\n",
    "                                           ], ignore_index=True)\n",
    "\n",
    "    def clean_textual_data(self):\n",
    "        \"\"\"\n",
    "            The next step is data cleaning. In this step the function \"apply_cleaning_functions\"\n",
    "            applies the following actions:\n",
    "                - clean the document from specific characters\n",
    "                - delete unicode\n",
    "                - removes emails and URLs and currency symbols\n",
    "        \"\"\"\n",
    "        self.documents_corpus = apply_cleaning_functions(self.documents_corpus)\n",
    "\n",
    "    def transform_to_spacy_doc(self):\n",
    "        \"\"\"\n",
    "            When the document is clean, is going to be transform into spacy document\n",
    "        \"\"\"\n",
    "        self.documents_corpus = self.documents_corpus.apply(nlp)\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "            To extract the parts of speech, below it was defined classes for each token is necessary.\n",
    "        \"\"\"\n",
    "\n",
    "        def doc_atomization_noun_phrases(doc: Doc):\n",
    "            \"\"\"\n",
    "\n",
    "            :param doc:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            sentence = str(doc)\n",
    "            for noun_phrase in doc.noun_chunks:\n",
    "                seq = str(noun_phrase)\n",
    "                sentence = sentence.replace(seq, seq.replace(' ', '_'))\n",
    "            return nlp(sentence)\n",
    "\n",
    "        self.documents_corpus = pd.concat([self.documents_corpus,\n",
    "                                           self.documents_corpus.apply(doc_atomization_noun_phrases)]\n",
    "                                          , ignore_index=True)\n",
    "        self.documents_corpus = self.documents_corpus.apply(lambda x: list(map(str, x)))\n",
    "\n",
    "    def model_training(self):\n",
    "        \"\"\"\n",
    "            When the data is prepared it's stored into Word2Vec model.\n",
    "        \"\"\"\n",
    "        self.word2vec = Word2Vec(sentences=self.documents_corpus, window=WINDOW,\n",
    "                                 min_count=MIN_COUNT, vector_size=VECTOR_SIZE)\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"\n",
    "            The final step is execution, where are stored each step and it will be executed in a row\n",
    "        \"\"\"\n",
    "        self.download_datasets()\n",
    "        self.extract_textual_data()\n",
    "        self.clean_textual_data()\n",
    "        self.transform_to_spacy_doc()\n",
    "        self.extract_features()\n",
    "        self.model_training()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class LanguageModelPipelineNouns(LanguageModelPipeline):\n",
    "    \"\"\"\n",
    "        It injects the LanguageModelPipeline method extract_pos,\n",
    "        that extracts NOUNs from the document\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_pos(self):\n",
    "        self.documents_corpus = self.documents_corpus.apply(select_pos, pos=\"NOUN\")\n",
    "        self.documents_corpus = self.documents_corpus.apply(lambda x: list(map(lambda docs: docs.lemma_, x)))\n",
    "\n",
    "\n",
    "class LanguageModelPipelineNounPhrases(LanguageModelPipeline):\n",
    "    \"\"\"\n",
    "        It injects the LanguageModelPipeline method extract_pos,\n",
    "        that extracts NOUN PHRASES from the document\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_pos(self):\n",
    "        self.documents_corpus = self.documents_corpus.apply(lambda x: x.noun_chunks)\n",
    "        self.documents_corpus = self.documents_corpus.apply(filter_stop_words_on_a_span_list)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Nr#1 language model based on:\n",
    "- PWDB\n",
    "- eu-timeline\n",
    "- ireland-timeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [
    "dataset_sources_config = [\n",
    "    #(Dataset.PWDB, PWDB_TEXTUAL_CLASS),\n",
    "    #(Dataset.EU_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN),\n",
    "    (Dataset.IRELAND_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN)\n",
    "]\n",
    "noun_language_model_pipeline = LanguageModelPipeline(dataset_sources=dataset_sources_config)\n",
    "noun_language_model_pipeline.execute()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "def euclidean_similarity(v1: np.array, v2: np.array) -> np.float:\n",
    "    \"\"\"\n",
    "\n",
    "    :param v1:\n",
    "    :param v2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    similarity_coefficient = 1 / (1 + np.linalg.norm(v1 - v2))\n",
    "    return similarity_coefficient\n",
    "\n",
    "def cosine_similarity(v1: np.array, v2: np.array) -> np.float:\n",
    "    \"\"\"\n",
    "\n",
    "    :param v1:\n",
    "    :param v2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    similarity_coefficient = np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    return similarity_coefficient\n",
    "\n",
    "\n",
    "def get_similarity_matrix(wv: KeyedVectors, similarity_function) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    :param wv:\n",
    "    :param similarity_function:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    similarity_matrix_columns = wv.index_to_key\n",
    "    similarity_matrix = pd.DataFrame(columns=similarity_matrix_columns)\n",
    "    for row_index in similarity_matrix_columns:\n",
    "        new_similarity_matrix_line = {column_index: similarity_function(wv[row_index], wv[column_index])\n",
    "                                      for column_index in similarity_matrix_columns\n",
    "                                      }\n",
    "        new_similarity_matrix_line = pd.DataFrame(new_similarity_matrix_line, index=[row_index])\n",
    "        similarity_matrix = similarity_matrix.append(new_similarity_matrix_line)\n",
    "    return similarity_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This implementation is much faster than the above implementation.\n",
    "\"\"\"\n",
    "def get_similarity_matrix_v1(wv: KeyedVectors, similarity_function) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    :param wv:\n",
    "    :param similarity_function:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    similarity_matrix_columns = wv.index_to_key\n",
    "    return pd.DataFrame([{column_index: similarity_function(wv[row_index], wv[column_index])\n",
    "                          for column_index in similarity_matrix_columns\n",
    "                          }\n",
    "                         for row_index in similarity_matrix_columns\n",
    "                         ], columns=similarity_matrix_columns, index=[similarity_matrix_columns])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.5 s, sys: 1.2 s, total: 39.7 s\n",
      "Wall time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = get_similarity_matrix_v1(wv=noun_language_model_pipeline.word2vec.wv, similarity_function=euclidean_similarity)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 115 ms, sys: 4.06 ms, total: 119 ms\n",
      "Wall time: 113 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = get_similarity_matrix_v1(wv=noun_language_model_pipeline.word2vec.wv, similarity_function=cosine_similarity)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.1 s, sys: 19.3 ms, total: 40.1 s\n",
      "Wall time: 40.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = get_similarity_matrix(wv=noun_language_model_pipeline.word2vec.wv, similarity_function=euclidean_similarity)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "data": {
      "text/plain": "                              for        to        of       and       the  \\\nfor                      1.000000  0.951911  0.947571  0.956258  0.908373   \nto                       0.951911  1.000000  0.951528  0.949225  0.899460   \nof                       0.947571  0.951528  1.000000  0.944124  0.885983   \nand                      0.956258  0.949225  0.944124  1.000000  0.913641   \nthe                      0.908373  0.899460  0.885983  0.913641  1.000000   \n...                           ...       ...       ...       ...       ...   \neuled                    0.786558  0.778600  0.768126  0.790537  0.843371   \nnext_step                0.787796  0.780350  0.769827  0.792169  0.845059   \nmarine_protected_areas   0.789761  0.782111  0.771583  0.794219  0.847532   \nireland's_maritime_area  0.786751  0.779499  0.768503  0.791248  0.844026   \nall_risk_groups          0.787543  0.780182  0.769329  0.792035  0.844608   \n\n                         minister        on        in         ,  announces  \\\nfor                      0.870570  0.915606  0.926763  0.921064   0.875706   \nto                       0.861631  0.906145  0.917454  0.912570   0.865935   \nof                       0.849024  0.894797  0.905088  0.899220   0.853596   \nand                      0.875805  0.922270  0.931306  0.926641   0.880517   \nthe                      0.933557  0.952978  0.949767  0.949722   0.937587   \n...                           ...       ...       ...       ...        ...   \neuled                    0.880878  0.837063  0.826050  0.831560   0.875723   \nnext_step                0.881369  0.838564  0.828269  0.832797   0.877183   \nmarine_protected_areas   0.884338  0.841262  0.829949  0.835792   0.879971   \nireland's_maritime_area  0.880768  0.837900  0.826724  0.832032   0.876315   \nall_risk_groups          0.881499  0.838521  0.827599  0.832823   0.877464   \n\n                         ...  social_farming      dogs  the_measurement  \\\nfor                      ...        0.787285  0.787541         0.787130   \nto                       ...        0.780127  0.780287         0.779763   \nof                       ...        0.769048  0.769471         0.768915   \nand                      ...        0.791921  0.792300         0.791907   \nthe                      ...        0.844992  0.844810         0.844702   \n...                      ...             ...       ...              ...   \neuled                    ...        0.955181  0.954625         0.954493   \nnext_step                ...        0.953788  0.952865         0.954011   \nmarine_protected_areas   ...        0.956592  0.955095         0.955567   \nireland's_maritime_area  ...        0.955867  0.953616         0.953385   \nall_risk_groups          ...        0.956006  0.955555         0.956936   \n\n                         broader_living_standards  initiatives     euled  \\\nfor                                      0.785744     0.788661  0.786558   \nto                                       0.778489     0.781207  0.778600   \nof                                       0.767766     0.770158  0.768126   \nand                                      0.790418     0.793042  0.790537   \nthe                                      0.843001     0.846351  0.843371   \n...                                           ...          ...       ...   \neuled                                    0.954521     0.954689  1.000000   \nnext_step                                0.954686     0.956742  0.952700   \nmarine_protected_areas                   0.958140     0.955732  0.956104   \nireland's_maritime_area                  0.955880     0.957022  0.954511   \nall_risk_groups                          0.956131     0.955446  0.955890   \n\n                         next_step  marine_protected_areas  \\\nfor                       0.787796                0.789761   \nto                        0.780350                0.782111   \nof                        0.769827                0.771583   \nand                       0.792169                0.794219   \nthe                       0.845059                0.847532   \n...                            ...                     ...   \neuled                     0.952700                0.956104   \nnext_step                 1.000000                0.952608   \nmarine_protected_areas    0.952608                1.000000   \nireland's_maritime_area   0.954621                0.954582   \nall_risk_groups           0.956171                0.955316   \n\n                         ireland's_maritime_area  all_risk_groups  \nfor                                     0.786751         0.787543  \nto                                      0.779499         0.780182  \nof                                      0.768503         0.769329  \nand                                     0.791248         0.792035  \nthe                                     0.844026         0.844608  \n...                                          ...              ...  \neuled                                   0.954511         0.955890  \nnext_step                               0.954621         0.956171  \nmarine_protected_areas                  0.954582         0.955316  \nireland's_maritime_area                 1.000000         0.957535  \nall_risk_groups                         0.957535         1.000000  \n\n[2036 rows x 2036 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>for</th>\n      <th>to</th>\n      <th>of</th>\n      <th>and</th>\n      <th>the</th>\n      <th>minister</th>\n      <th>on</th>\n      <th>in</th>\n      <th>,</th>\n      <th>announces</th>\n      <th>...</th>\n      <th>social_farming</th>\n      <th>dogs</th>\n      <th>the_measurement</th>\n      <th>broader_living_standards</th>\n      <th>initiatives</th>\n      <th>euled</th>\n      <th>next_step</th>\n      <th>marine_protected_areas</th>\n      <th>ireland's_maritime_area</th>\n      <th>all_risk_groups</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>for</th>\n      <td>1.000000</td>\n      <td>0.951911</td>\n      <td>0.947571</td>\n      <td>0.956258</td>\n      <td>0.908373</td>\n      <td>0.870570</td>\n      <td>0.915606</td>\n      <td>0.926763</td>\n      <td>0.921064</td>\n      <td>0.875706</td>\n      <td>...</td>\n      <td>0.787285</td>\n      <td>0.787541</td>\n      <td>0.787130</td>\n      <td>0.785744</td>\n      <td>0.788661</td>\n      <td>0.786558</td>\n      <td>0.787796</td>\n      <td>0.789761</td>\n      <td>0.786751</td>\n      <td>0.787543</td>\n    </tr>\n    <tr>\n      <th>to</th>\n      <td>0.951911</td>\n      <td>1.000000</td>\n      <td>0.951528</td>\n      <td>0.949225</td>\n      <td>0.899460</td>\n      <td>0.861631</td>\n      <td>0.906145</td>\n      <td>0.917454</td>\n      <td>0.912570</td>\n      <td>0.865935</td>\n      <td>...</td>\n      <td>0.780127</td>\n      <td>0.780287</td>\n      <td>0.779763</td>\n      <td>0.778489</td>\n      <td>0.781207</td>\n      <td>0.778600</td>\n      <td>0.780350</td>\n      <td>0.782111</td>\n      <td>0.779499</td>\n      <td>0.780182</td>\n    </tr>\n    <tr>\n      <th>of</th>\n      <td>0.947571</td>\n      <td>0.951528</td>\n      <td>1.000000</td>\n      <td>0.944124</td>\n      <td>0.885983</td>\n      <td>0.849024</td>\n      <td>0.894797</td>\n      <td>0.905088</td>\n      <td>0.899220</td>\n      <td>0.853596</td>\n      <td>...</td>\n      <td>0.769048</td>\n      <td>0.769471</td>\n      <td>0.768915</td>\n      <td>0.767766</td>\n      <td>0.770158</td>\n      <td>0.768126</td>\n      <td>0.769827</td>\n      <td>0.771583</td>\n      <td>0.768503</td>\n      <td>0.769329</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>0.956258</td>\n      <td>0.949225</td>\n      <td>0.944124</td>\n      <td>1.000000</td>\n      <td>0.913641</td>\n      <td>0.875805</td>\n      <td>0.922270</td>\n      <td>0.931306</td>\n      <td>0.926641</td>\n      <td>0.880517</td>\n      <td>...</td>\n      <td>0.791921</td>\n      <td>0.792300</td>\n      <td>0.791907</td>\n      <td>0.790418</td>\n      <td>0.793042</td>\n      <td>0.790537</td>\n      <td>0.792169</td>\n      <td>0.794219</td>\n      <td>0.791248</td>\n      <td>0.792035</td>\n    </tr>\n    <tr>\n      <th>the</th>\n      <td>0.908373</td>\n      <td>0.899460</td>\n      <td>0.885983</td>\n      <td>0.913641</td>\n      <td>1.000000</td>\n      <td>0.933557</td>\n      <td>0.952978</td>\n      <td>0.949767</td>\n      <td>0.949722</td>\n      <td>0.937587</td>\n      <td>...</td>\n      <td>0.844992</td>\n      <td>0.844810</td>\n      <td>0.844702</td>\n      <td>0.843001</td>\n      <td>0.846351</td>\n      <td>0.843371</td>\n      <td>0.845059</td>\n      <td>0.847532</td>\n      <td>0.844026</td>\n      <td>0.844608</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>euled</th>\n      <td>0.786558</td>\n      <td>0.778600</td>\n      <td>0.768126</td>\n      <td>0.790537</td>\n      <td>0.843371</td>\n      <td>0.880878</td>\n      <td>0.837063</td>\n      <td>0.826050</td>\n      <td>0.831560</td>\n      <td>0.875723</td>\n      <td>...</td>\n      <td>0.955181</td>\n      <td>0.954625</td>\n      <td>0.954493</td>\n      <td>0.954521</td>\n      <td>0.954689</td>\n      <td>1.000000</td>\n      <td>0.952700</td>\n      <td>0.956104</td>\n      <td>0.954511</td>\n      <td>0.955890</td>\n    </tr>\n    <tr>\n      <th>next_step</th>\n      <td>0.787796</td>\n      <td>0.780350</td>\n      <td>0.769827</td>\n      <td>0.792169</td>\n      <td>0.845059</td>\n      <td>0.881369</td>\n      <td>0.838564</td>\n      <td>0.828269</td>\n      <td>0.832797</td>\n      <td>0.877183</td>\n      <td>...</td>\n      <td>0.953788</td>\n      <td>0.952865</td>\n      <td>0.954011</td>\n      <td>0.954686</td>\n      <td>0.956742</td>\n      <td>0.952700</td>\n      <td>1.000000</td>\n      <td>0.952608</td>\n      <td>0.954621</td>\n      <td>0.956171</td>\n    </tr>\n    <tr>\n      <th>marine_protected_areas</th>\n      <td>0.789761</td>\n      <td>0.782111</td>\n      <td>0.771583</td>\n      <td>0.794219</td>\n      <td>0.847532</td>\n      <td>0.884338</td>\n      <td>0.841262</td>\n      <td>0.829949</td>\n      <td>0.835792</td>\n      <td>0.879971</td>\n      <td>...</td>\n      <td>0.956592</td>\n      <td>0.955095</td>\n      <td>0.955567</td>\n      <td>0.958140</td>\n      <td>0.955732</td>\n      <td>0.956104</td>\n      <td>0.952608</td>\n      <td>1.000000</td>\n      <td>0.954582</td>\n      <td>0.955316</td>\n    </tr>\n    <tr>\n      <th>ireland's_maritime_area</th>\n      <td>0.786751</td>\n      <td>0.779499</td>\n      <td>0.768503</td>\n      <td>0.791248</td>\n      <td>0.844026</td>\n      <td>0.880768</td>\n      <td>0.837900</td>\n      <td>0.826724</td>\n      <td>0.832032</td>\n      <td>0.876315</td>\n      <td>...</td>\n      <td>0.955867</td>\n      <td>0.953616</td>\n      <td>0.953385</td>\n      <td>0.955880</td>\n      <td>0.957022</td>\n      <td>0.954511</td>\n      <td>0.954621</td>\n      <td>0.954582</td>\n      <td>1.000000</td>\n      <td>0.957535</td>\n    </tr>\n    <tr>\n      <th>all_risk_groups</th>\n      <td>0.787543</td>\n      <td>0.780182</td>\n      <td>0.769329</td>\n      <td>0.792035</td>\n      <td>0.844608</td>\n      <td>0.881499</td>\n      <td>0.838521</td>\n      <td>0.827599</td>\n      <td>0.832823</td>\n      <td>0.877464</td>\n      <td>...</td>\n      <td>0.956006</td>\n      <td>0.955555</td>\n      <td>0.956936</td>\n      <td>0.956131</td>\n      <td>0.955446</td>\n      <td>0.955890</td>\n      <td>0.956171</td>\n      <td>0.955316</td>\n      <td>0.957535</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>2036 rows Ã— 2036 columns</p>\n</div>"
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Nr#2 language model based on:\n",
    "- eu-cellar\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2653 of 2653) |####################| Elapsed Time: 0:00:03 Time:  0:00:03\n"
     ]
    }
   ],
   "source": [
    "dataset_sources_config = [\n",
    "    (Dataset.EU_CELLAR, DEFAULT_TEXTUAL_COLUMN)\n",
    "]\n",
    "noun_language_model_pipeline = LanguageModelPipelineNouns(dataset_sources=dataset_sources_config)\n",
    "noun_language_model_pipeline.execute()\n",
    "\n",
    "noun_phrases_language_model_pipeline = LanguageModelPipelineNounPhrases(dataset_sources=dataset_sources_config)\n",
    "noun_phrases_language_model_pipeline.execute()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Nr#3 language model based on:\n",
    "- PWDB\n",
    "- eu-timeline\n",
    "- ireland-timeline\n",
    "- eu-cellar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "dataset_sources_config = [\n",
    "    (Dataset.PWDB, PWDB_TEXTUAL_CLASS),\n",
    "    (Dataset.EU_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN),\n",
    "    (Dataset.IRELAND_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN),\n",
    "    (Dataset.EU_CELLAR, DEFAULT_TEXTUAL_COLUMN)\n",
    "]\n",
    "noun_language_model_pipeline = LanguageModelPipelineNouns(dataset_sources=dataset_sources_config)\n",
    "noun_language_model_pipeline.execute()\n",
    "\n",
    "noun_phrases_language_model_pipeline = LanguageModelPipelineNounPhrases(dataset_sources=dataset_sources_config)\n",
    "noun_phrases_language_model_pipeline.execute()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}