{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Word2vec model training\n",
    "#### Model training based on three datasets' text data:\n",
    "- M1: pwdb + eu_timeline  ( +  ireland_timeline )\n",
    "- M2: ds_eu_cellar\n",
    "- M3: M1+M2\n",
    "\n",
    "#### Extract NOUN and NOUN PHRASES from each text data\n",
    "#### Train the word2vec model with each dataset's textual data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/jovyan/work/sem-covid/\")\n",
    "sys.path = list(set(sys.path))\n",
    "\n",
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir('/home/jovyan/work/sem-covid/')\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "from typing import List, Tuple\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 1500000\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from d3graph import d3graph\n",
    "\n",
    "from sem_covid.services.data_registry import Dataset\n",
    "from sem_covid.services.store_registry import store_registry\n",
    "from sem_covid.adapters.data_source import IndexTabularDataSource\n",
    "\n",
    "from sem_covid.entrypoints.notebooks.topic_modeling.topic_modeling_wrangling.token_management import select_pos\n",
    "\n",
    "from sem_covid.services.sc_wrangling.data_cleaning import (clean_text_from_specific_characters, clean_fix_unicode,\n",
    "                                                           clean_remove_currency_symbols, clean_remove_emails,\n",
    "                                                           clean_remove_urls, clean_remove_stopwords)\n",
    "\n",
    "from sem_covid.entrypoints.notebooks.language_modeling.language_model_tools.similarity_calculus import build_similarity_matrix\n",
    "\n",
    "from sem_covid.entrypoints.notebooks.language_modeling.language_model_tools.document_handling_tools import (\n",
    "    document_atomization_noun_phrases, lemmatize_document)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "PWDB_TEXTUAL_CLASS = ['title', 'background_info_description', 'content_of_measure_description',\n",
    "                      'use_of_measure_description', 'involvement_of_social_partners_description']\n",
    "\n",
    "DEFAULT_TEXTUAL_COLUMN = ['title']\n",
    "WINDOW = 5\n",
    "MIN_COUNT = 1\n",
    "VECTOR_SIZE = 300\n",
    "EPOCHS = 50\n",
    "EU_TIMELINE_TOTAL_EXAMPLES = 171\n",
    "IRELAND_TIMELINE_TOTAL_EXAMPLES = 410\n",
    "EU_CELLAR_TOTAL_EXAMPLES = 2653\n",
    "\n",
    "KEY_WORDS = ['work', 'agreement', 'working', 'companies', 'workers',\n",
    "             'measures', 'temporary', 'social', 'support', 'covid19',\n",
    "             '2020', 'public', 'national', 'ireland', 'statement', '2021',\n",
    "             'announce', 'health', 'minister', 'new', 'billion', 'coronavirus',\n",
    "             'vaccine', 'eur', 'support', 'million', 'commission', 'eu']\n",
    "\n",
    "COUNTRIES = ['austria', 'belgium', 'bulgaria', 'croatia', 'cyprus', 'czechia', 'denmark', 'estonia',\n",
    "             'european_union', 'finland', 'france', 'germany', 'greece', 'hungary', 'ireland', 'italy',\n",
    "             'latvia', 'lithuania', 'luxembourg', 'malta', 'netherlands', 'norway', 'poland', 'portugal',\n",
    "             'romania', 'slovakia', 'slovenia', 'spain', 'sweden', 'united', 'kingdom']\n",
    "\n",
    "CATEGORY = ['retention', 'workplace', 'labour', 'recovery', 'economic', 'adaptation',\n",
    "            'businesses', 'protection', 'essential', 'workers', 'business_continuity',\n",
    "            'services', 'social', 'market']\n",
    "\n",
    "SUBCATEGORY = ['safety', 'arrangements', 'health', 'spending', 'working', 'support', 'occupational',\n",
    "               'stimulus_packages', 'access', 'time', 'finance', 'remote', 'flexibility', 'workers',\n",
    "               'essential_services', 'remuneration']\n",
    "\n",
    "TARGET_GROUPS_L1 = ['businesses', 'workers', 'citizens']\n",
    "\n",
    "TARGET_GROUPS_L2 = ['company', 'older', 'people', 'female', 'aged', 'corporations', 'businesses',\n",
    "                    'single', 'person', 'larger', 'forms', 'smes', 'ups', 'non', 'single_parents',\n",
    "                    'citizens', 'professions', 'parents', 'groups', 'youth', 'workers', 'essential_services',\n",
    "                    'sector', 'women', 'workplace', 'unemployed', 'care', 'facilities', 'other', 'standard',\n",
    "                    'specific', 'companies', 'self', 'contractors', 'children', 'border', 'solo', 'refugees',\n",
    "                    'minors', 'cross', 'platform', 'employment', 'seasonal', 'disabled', 'migrants', 'start',\n",
    "                    'risk_group', 'commuters', 'employees']\n",
    "\n",
    "FUNDING = ['companies', 'national_funds', 'employer', 'funds', 'national_funds']\n",
    "\n",
    "WORD_GRAPH_CONFIGS = {'category': CATEGORY,\n",
    "                      'subcategory': SUBCATEGORY,\n",
    "                      'key_words': KEY_WORDS,\n",
    "                      'countries': COUNTRIES,\n",
    "                      'target_groups_l1': TARGET_GROUPS_L1,\n",
    "                      'target_groups_l2': TARGET_GROUPS_L2,\n",
    "                      'funding': FUNDING\n",
    "                      }\n",
    "\n",
    "NR1_MODEL_NAME = 'model1'\n",
    "NR2_MODEL_NAME = 'model2'\n",
    "NR3_MODEL_NAME = 'model3'\n",
    "\n",
    "MODEL1_FILE_NAME = 'model1_language_model.model'\n",
    "MODEL2_FILE_NAME = 'model2_language_model.model'\n",
    "MODEL3_FILE_NAME = 'model3_language_model.model'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preprocessing\n",
    "- data cleanup\n",
    "- turn corpus into spacy document"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def add_space_between_dots_and_commas(text: str):\n",
    "    return re.sub(r'(?<=[.,])(?=[^\\s])', r' ', text)\n",
    "\n",
    "\n",
    "def apply_cleaning_functions(document_corpus: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    This function receives the document and leads through cleaning steps\n",
    "    Args:\n",
    "        document_corpus: dataset document corpus\n",
    "\n",
    "    Returns: clean document corpus\n",
    "    \"\"\"\n",
    "    unused_characters = [\"\\\\r\", \">\", \"\\n\", \"\\\\\", \"<\", \"''\", \"%\", \"...\", \"\\'\", '\"', \"(\", \"\\n\", \"*\", \"1)\", \"2)\", \"3)\",\n",
    "                         \"[\", \"]\", \"-\", \"_\", \"\\r\", '®', '..']\n",
    "\n",
    "    new_document_corpus = document_corpus.apply(clean_text_from_specific_characters, characters=unused_characters)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_fix_unicode)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_urls)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_emails)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_currency_symbols)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_stopwords)\n",
    "    new_document_corpus = new_document_corpus.apply(add_space_between_dots_and_commas)\n",
    "\n",
    "    return new_document_corpus\n",
    "\n",
    "\n",
    "def generate_graph(similarity_matrix: pd.DataFrame, graph: nx.Graph, root_word: str,\n",
    "                   top_words: int, threshold: np.float64 = 0.8, deep_level: int = 0,\n",
    "                   max_deep_level: int = 2, deep_map: dict = None, color_map: dict = None) -> nx.Graph:\n",
    "    \"\"\"\n",
    "        Generates d3 graph using the inserted keywords and their top words from similarity matrix\n",
    "    Args:\n",
    "        similarity_matrix: Dataframe with word similarity\n",
    "        graph: networkx graph\n",
    "        root_word: key words\n",
    "        top_words: top similar words from inserted keywords\n",
    "        threshold: minimum percentage of similarity\n",
    "        deep_level: the level of generating leaf\n",
    "        max_deep_level: the maximum number of generated leaf\n",
    "        deep_map: dictionary of the words and their level of similarity\n",
    "        color_map: the color of each level of words' similarity\n",
    "\n",
    "    Returns: a d3 graph with title and root of key word and their similarity words\n",
    "    \"\"\"\n",
    "    if root_word not in deep_map.keys():\n",
    "        deep_map[root_word] = (deep_level, color_map[deep_level])\n",
    "    elif deep_map[root_word][0] > deep_level:\n",
    "        deep_map[root_word] = (deep_level, color_map[deep_level])\n",
    "    if deep_level > max_deep_level:\n",
    "        return graph\n",
    "    new_nodes = similarity_matrix[root_word].sort_values(ascending=False)[:top_words].index.to_list()\n",
    "    new_nodes_weight = list(similarity_matrix[root_word].sort_values(ascending=False)[:top_words].values)\n",
    "    for index in range(0, len(new_nodes)):\n",
    "        if new_nodes_weight[index] >= threshold:\n",
    "            graph.add_edge(root_word, new_nodes[index])\n",
    "            generate_graph(similarity_matrix, graph, new_nodes[index], top_words, threshold, deep_level + 1,\n",
    "                           max_deep_level,\n",
    "                           deep_map=deep_map, color_map=color_map)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def create_graph_for_language_model_key_words(similarity_matrix: pd.DataFrame, language_model_words: list,\n",
    "                                              model_name: str, column_name: str,\n",
    "                                              metric_threshold: np.float64) -> d3graph:\n",
    "    \"\"\"\n",
    "    !!! This is not reusable function. It was made for a single thing !!!\n",
    "\n",
    "    It generates d3graph based on language model selected words and and the similarity\n",
    "    matrix created with those words.\n",
    "    \"\"\"\n",
    "    graph_folder_path = f'docs/word-similarity-web/{model_name}_graphs/{column_name}/'\n",
    "    color_map = {0: '#a70000',\n",
    "                 1: '#f0000',\n",
    "                 2: '#ff7b7b',\n",
    "                 3: '#ffbaba'}\n",
    "    for index in range(0, len(language_model_words)):\n",
    "        deep_map = {}\n",
    "        graph = generate_graph(similarity_matrix, nx.Graph(), language_model_words[index],\n",
    "                               top_words=4, threshold=metric_threshold\n",
    "                               , max_deep_level=2, deep_map=deep_map, color_map=color_map)\n",
    "        network_adjacency_matrix = pd.DataFrame(data=nx.adjacency_matrix(graph).todense(),\n",
    "                                                index=graph.nodes(), columns=graph.nodes())\n",
    "        node_color_list = [deep_map[node][0] for node in graph.nodes()]\n",
    "        d3graph(network_adjacency_matrix, savepath=graph_folder_path, savename=language_model_words[index],\n",
    "                node_color=node_color_list,\n",
    "                width=1920, height=1080, edge_width=5,\n",
    "                edge_distance=60, directed=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class LanguageModelPipeline:\n",
    "    \"\"\"\n",
    "        This pipeline executes the steps for word2vec language training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_sources: List[Tuple[IndexTabularDataSource, List[str]]], language_model_name: str):\n",
    "        \"\"\"\n",
    "            :param dataset_sources: represents the source of the datasets.\n",
    "        \"\"\"\n",
    "        self.dataset_sources = dataset_sources\n",
    "        self.language_model_name = language_model_name\n",
    "        self.documents_corpus = pd.Series()\n",
    "        self.word2vec = None\n",
    "\n",
    "    def download_datasets(self):\n",
    "        \"\"\"\n",
    "            In this step it will download the dataset and detect selected columns.\n",
    "            It can be downloaded as many datasets as there are in data source.\n",
    "        \"\"\"\n",
    "        self.dataset_sources = [(dataset_columns, dataset_source.fetch())\n",
    "                                for dataset_source, dataset_columns in self.dataset_sources]\n",
    "\n",
    "    def extract_textual_data(self):\n",
    "        \"\"\"\n",
    "            After downloading the datasets, the textual data will be found and and concatenated\n",
    "            with executing of several steps as well. It will fill the NaN values with empty space,\n",
    "            add a dot at the end of each concatenated column and reset the index.\n",
    "        \"\"\"\n",
    "        self.documents_corpus = pd.concat([dataset[columns]\n",
    "                                          .fillna(value=\"\")\n",
    "                                          .agg('. '.join, axis=1)\n",
    "                                          .reset_index(drop=True)\n",
    "                                           for columns, dataset in self.dataset_sources\n",
    "                                           ], ignore_index=True)\n",
    "\n",
    "    def clean_textual_data(self):\n",
    "        \"\"\"\n",
    "            The next step is data cleaning. In this step the function \"apply_cleaning_functions\"\n",
    "            applies the following actions:\n",
    "                - clean the document from specific characters\n",
    "                - delete unicode\n",
    "                - removes emails and URLs and currency symbols\n",
    "        \"\"\"\n",
    "        self.documents_corpus = apply_cleaning_functions(self.documents_corpus)\n",
    "\n",
    "    def transform_to_spacy_doc(self):\n",
    "        \"\"\"\n",
    "            When the document is clean, is going to be transform into spacy document\n",
    "        \"\"\"\n",
    "        self.documents_corpus = self.documents_corpus.apply(nlp)\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "            To extract the parts of speech, below it was defined classes for each token is necessary.\n",
    "        \"\"\"\n",
    "        self.documents_corpus = pd.concat([self.documents_corpus,\n",
    "                                           self.documents_corpus.apply(document_atomization_noun_phrases),\n",
    "                                           self.documents_corpus.apply(lemmatize_document)]\n",
    "                                          , ignore_index=True)\n",
    "\n",
    "        self.documents_corpus = self.documents_corpus.apply(lambda x: list(map(str, x)))\n",
    "\n",
    "    def model_training(self):\n",
    "        \"\"\"\n",
    "            When the data is prepared it's stored into Word2Vec model.\n",
    "        \"\"\"\n",
    "        self.word2vec = Word2Vec(sentences=self.documents_corpus, window=WINDOW,\n",
    "                                 min_count=MIN_COUNT, size=VECTOR_SIZE)\n",
    "\n",
    "    def save_language_model(self):\n",
    "        \"\"\"\n",
    "            Saves trained model in MinIO\n",
    "        \"\"\"\n",
    "        minio = store_registry.minio_object_store('mdl-language')\n",
    "        minio.put_object('word2vec/' + self.language_model_name, pickle.dumps(self.word2vec))\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"\n",
    "            The final step is execution, where are stored each step and it will be executed in a row\n",
    "        \"\"\"\n",
    "        self.download_datasets()\n",
    "        self.extract_textual_data()\n",
    "        self.clean_textual_data()\n",
    "        self.transform_to_spacy_doc()\n",
    "        self.extract_features()\n",
    "        self.model_training()\n",
    "        self.save_language_model()\n",
    "\n",
    "\n",
    "class LanguageModelWordsFilter:\n",
    "    def __init__(self, word2vec_model: Word2Vec, key_words: List[str], pos: List[str]) -> None:\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.key_words = key_words\n",
    "        self.pos = pos\n",
    "        self.word2vec_document = None\n",
    "        self.word2vec_document = nlp(' '.join(self.word2vec_model.wv.index2word))\n",
    "        self.word2vec_document = select_pos(self.word2vec_document, self.pos)\n",
    "        self._extract_pos = list(map(str, self.word2vec_document))\n",
    "\n",
    "    def extract_pos(self) -> List[str]:\n",
    "        \"\"\"\n",
    "            transforms a word2vec indexes into spacy document and selects parts of\n",
    "            speech. After that it puts into a list and converts those parts of speech\n",
    "            into strings.\n",
    "        \"\"\"\n",
    "        return self._extract_pos\n",
    "\n",
    "    def select_key_words(self) -> List[str]:\n",
    "        \"\"\"\n",
    "            Finds each word form inserted list of key words and returns a\n",
    "            list with those words if there are presented in the list of\n",
    "            extracted parts of speech.\n",
    "        \"\"\"\n",
    "        return [word for word in self.key_words if word in self._extract_pos]\n",
    "\n",
    "    def select_pos_index(self) -> List[int]:\n",
    "        \"\"\"\n",
    "            Detects the part of speech indexes and returns them into a list\n",
    "        \"\"\"\n",
    "        return [self.word2vec_model.wv.index2word.index(token) for token in self._extract_pos\n",
    "                if token in self.word2vec_model.wv.index2word]\n",
    "\n",
    "    def select_pos_embeddings(self) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "            Detects part of speech embeddings from their indexes\n",
    "        \"\"\"\n",
    "        selected_pos_index = self.select_pos_index()\n",
    "        return [self.word2vec_model.wv.vectors[index] for index in selected_pos_index]\n",
    "\n",
    "\n",
    "class LanguageModelExecutionSteps:\n",
    "    def __init__(self, language_model_file_name: str, model_name: str) -> None:\n",
    "        \"\"\"\n",
    "            This class executes three steps to view the similarity between words from word2vec model:\n",
    "            (1) trains language model\n",
    "            (2) filters language model words\n",
    "            (3) trains similarity matrices\n",
    "            Args:\n",
    "                language_model_file_name: word2vec model filename to be stored in MinIO\n",
    "                model_name: the name of the model config\n",
    "        \"\"\"\n",
    "        self.language_model_file_name = language_model_file_name\n",
    "        self.model_name = model_name\n",
    "        self.word2vec = None\n",
    "\n",
    "    def train_language_model(self, dataset_sources_config: List[tuple]) -> None:\n",
    "        \"\"\"\n",
    "            Trains word2vec model from dataset config,\n",
    "            which is a list of tuples of dataset and their textual columns.\n",
    "            After training the model is stored in MinIO for later use.\n",
    "        \"\"\"\n",
    "        model_language_model_pipeline = LanguageModelPipeline(dataset_sources=dataset_sources_config,\n",
    "                                                              language_model_name=self.language_model_file_name)\n",
    "        model_language_model_pipeline.execute()\n",
    "        self.word2vec = model_language_model_pipeline.word2vec\n",
    "\n",
    "    def filter_language_model_words(self, key_words: List[str]):\n",
    "        \"\"\"\n",
    "            It filters the word2vec indexes, extracting selected noun phrases\n",
    "        \"\"\"\n",
    "        if self.word2vec:\n",
    "            return LanguageModelWordsFilter(self.word2vec, key_words, pos=['NOUN', 'ADJ'])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def train_similarity_matrices(self, key_words: List[str]) -> None:\n",
    "        \"\"\"\n",
    "            for each type of similarity functions, it will create a similarity matrix, based on\n",
    "            indexes and their vectors from most similar words of key words.\n",
    "            The matrices will be stored in MinIO for later use.\n",
    "        \"\"\"\n",
    "        similarity_functions = ['cosine', 'euclidean', 'hamming']\n",
    "        for index in range(len(similarity_functions)):\n",
    "            print('Start computing similarity matrix.')\n",
    "            model_similarity_matrix = build_similarity_matrix(\n",
    "                self.filter_language_model_words(key_words=key_words).select_pos_embeddings(),\n",
    "                self.filter_language_model_words(key_words=key_words).extract_pos(),\n",
    "                metric=similarity_functions[index])\n",
    "            print('Finish computing similarity matrix.')\n",
    "            print('Save similarity matrix.')\n",
    "            store_registry.minio_object_store('semantic-similarity-matrices').put_object(\n",
    "                f'{self.model_name}_{similarity_functions[index]}_matrix.json',\n",
    "                model_similarity_matrix.to_json(orient='columns'))\n",
    "\n",
    "\n",
    "def plot_graphs(pipeline: LanguageModelExecutionSteps, model_name: str, model_file_name: str, threshold: np.float64,\n",
    "                word_graph_configs: dict, normalize_func) -> None:\n",
    "    \"\"\"\n",
    "        steps of generating d3 graph, calling the similarity matrix from minio and normalizing it.\n",
    "    Args:\n",
    "        pipeline: Pipeline of language model execution stems\n",
    "        model_name: the name of the model\n",
    "        model_file_name: word2vec file name from MinIO\n",
    "        threshold: the minimum of similarity number\n",
    "        word_graph_configs: dictionary of key words\n",
    "        normalize_func: function of similarity normalization\n",
    "    \"\"\"\n",
    "    model_cosine_matrix = store_registry.minio_object_store('semantic-similarity-matrices').get_object(model_file_name)\n",
    "    for key in word_graph_configs.keys():\n",
    "        create_graph_for_language_model_key_words(pd.read_json(model_cosine_matrix).applymap(normalize_func),\n",
    "                                                  pipeline.filter_language_model_words(\n",
    "                                                      key_words=word_graph_configs[key]).select_key_words(),\n",
    "                                                  model_name=model_name, metric_threshold=threshold,\n",
    "                                                  column_name=key)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Nr#1 language model based on:\n",
    "- PWDB\n",
    "- eu-timeline\n",
    "- ireland-timeline\n",
    "\n",
    "## Experiment Nr#2 language model based on:\n",
    "- eu-cellar\n",
    "\n",
    "## Experiment Nr#3 language model based on:\n",
    "- PWDB\n",
    "- eu-timeline\n",
    "- ireland-timeline\n",
    "- eu-cellar\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model1_dataset_sources_config = [\n",
    "    (Dataset.PWDB, PWDB_TEXTUAL_CLASS),\n",
    "    (Dataset.EU_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN),\n",
    "    (Dataset.IRELAND_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN)\n",
    "]\n",
    "\n",
    "model2_dataset_sources_config = [\n",
    "    (Dataset.EU_CELLAR_ENRICHED, DEFAULT_TEXTUAL_COLUMN),\n",
    "]\n",
    "\n",
    "model3_dataset_sources_config = [\n",
    "    (Dataset.PWDB, PWDB_TEXTUAL_CLASS),\n",
    "    (Dataset.EU_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN),\n",
    "    (Dataset.EU_CELLAR_ENRICHED, DEFAULT_TEXTUAL_COLUMN),\n",
    "    (Dataset.IRELAND_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1288 of 1288) |####################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (171 of 171) |######################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (410 of 410) |######################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "model1_execution_steps = LanguageModelExecutionSteps(language_model_file_name=MODEL1_FILE_NAME, model_name=NR1_MODEL_NAME)\n",
    "model1_execution_steps.train_language_model(model1_dataset_sources_config)\n",
    "model1_execution_steps.train_similarity_matrices(CATEGORY)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2653 of 2653) |####################| Elapsed Time: 0:00:01 Time:  0:00:01\n"
     ]
    }
   ],
   "source": [
    "model2_execution_steps = LanguageModelExecutionSteps(language_model_file_name=MODEL2_FILE_NAME, model_name=NR2_MODEL_NAME)\n",
    "model2_execution_steps.train_language_model(model2_dataset_sources_config)\n",
    "model2_execution_steps.train_similarity_matrices(CATEGORY)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2653 of 2653) |####################| Elapsed Time: 0:00:01 Time:  0:00:01\n"
     ]
    }
   ],
   "source": [
    "model3_execution_steps = LanguageModelExecutionSteps(language_model_file_name=MODEL3_FILE_NAME, model_name=NR3_MODEL_NAME)\n",
    "model3_execution_steps.train_language_model(model3_dataset_sources_config)\n",
    "model3_execution_steps.train_similarity_matrices(CATEGORY)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate D3 Graphs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cosine similarity graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starts protting graph\n",
      "[d3graph] >Creating directory [docs/word-similarity-web/model1_graphs/category/]\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/category/workplace.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/category/labour.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/category/recovery.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/category/economic.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/category/adaptation.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/category/businesses.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/category/essential.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/category/workers.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/category/services.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/category/social.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/category/market.html\n",
      "[d3graph] >Creating directory [docs/word-similarity-web/model1_graphs/subcategory/]\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/subcategory/arrangements.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/subcategory/support.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/subcategory/occupational.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/subcategory/access.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/subcategory/time.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/subcategory/finance.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/subcategory/flexibility.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/subcategory/workers.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/subcategory/remuneration.html\n",
      "[d3graph] >Creating directory [docs/word-similarity-web/model1_graphs/key_words/]\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/key_words/work.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/key_words/agreement.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/key_words/companies.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/key_words/workers.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/key_words/measures.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/key_words/temporary.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/key_words/social.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/key_words/support.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/key_words/public.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/key_words/national.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/key_words/statement.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/key_words/announce.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/key_words/new.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/key_words/vaccine.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/key_words/eur.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/key_words/support.html\n",
      "[d3graph] >Creating directory [docs/word-similarity-web/model1_graphs/countries/]\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/countries/belgium.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/countries/croatia.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/countries/czechia.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/countries/denmark.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/countries/finland.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/countries/luxembourg.html\n",
      "[d3graph] >Creating directory [docs/word-similarity-web/model1_graphs/target_groups_l1/]\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l1/businesses.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l1/workers.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l1/citizens.html\n",
      "[d3graph] >Creating directory [docs/word-similarity-web/model1_graphs/target_groups_l2/]\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/company.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/older.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/people.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/female.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/aged.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/corporations.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/businesses.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/single.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/person.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/larger.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/forms.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/smes.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/ups.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/non.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/citizens.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/professions.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/parents.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/groups.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/youth.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/workers.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/sector.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/women.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/workplace.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/unemployed.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/care.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/other.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/standard.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/specific.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/companies.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/self.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/contractors.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/children.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/border.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/solo.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/refugees.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/minors.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/platform.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/seasonal.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/disabled.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/migrants.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/commuters.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/target_groups_l2/employees.html\n",
      "[d3graph] >Creating directory [docs/word-similarity-web/model1_graphs/funding/]\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/funding/companies.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/funding/employer.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model1_graphs/funding/funds.html\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print('starts protting graph')\n",
    "plot_graphs(pipeline=model1_execution_steps,\n",
    "            model_name=NR1_MODEL_NAME,\n",
    "            model_file_name='model1_cosine_matrix.json',\n",
    "            threshold=0.6,\n",
    "            word_graph_configs=WORD_GRAPH_CONFIGS,\n",
    "            normalize_func=lambda x: 1 - x)\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_graphs(pipeline=model2_execution_steps,\n",
    "            model_name=NR2_MODEL_NAME,\n",
    "            model_file_name='model2_cosine_matrix.json',\n",
    "            threshold=0.6,\n",
    "            word_graph_configs=WORD_GRAPH_CONFIGS,\n",
    "            normalize_func=lambda x: 1 - x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starts protting graph\n"
     ]
    }
   ],
   "source": [
    "print('starts protting graph')\n",
    "plot_graphs(pipeline=model3_execution_steps,\n",
    "            model_name=NR3_MODEL_NAME,\n",
    "            model_file_name='model3_cosine_matrix.json',\n",
    "            threshold=0.6,\n",
    "            word_graph_configs=WORD_GRAPH_CONFIGS,\n",
    "            normalize_func=lambda x: 1 - x)\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Pentru a normaliza o distanță spațială într-o metrică de similaritate\n",
    "# aplică formula dată:\n",
    "# sim_metric = 1/(1+distance)\n",
    "# unde sim_metric este distanța normalizată, ce reprezintă metrica de similaritate\n",
    "# distance - este distanța calculată după o metodă anume( ex. Eucliedean, norma l1, norma l2, norma infint, etc.)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}