{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Word2vec model training\n",
    "#### Model training based on three datasets' text data:\n",
    "- M1: pwdb + eu_timeline  ( +  ireland_timeline )\n",
    "- M2: ds_eu_cellar\n",
    "- M3: M1+M2\n",
    "\n",
    "#### Extract NOUN and NOUN PHRASES from each text data\n",
    "#### Train the word2vec model with each dataset's textual data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/jovyan/work/sem-covid/\")\n",
    "sys.path = list(set(sys.path))\n",
    "\n",
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir('/home/jovyan/work/sem-covid/')\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "from typing import List, Tuple\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 1500000\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from d3graph import d3graph\n",
    "\n",
    "from sem_covid.services.data_registry import Dataset\n",
    "from sem_covid.services.store_registry import store_registry\n",
    "from sem_covid.adapters.data_source import IndexTabularDataSource\n",
    "\n",
    "from sem_covid.entrypoints.notebooks.topic_modeling.topic_modeling_wrangling.token_management import select_pos\n",
    "\n",
    "from sem_covid.services.sc_wrangling.data_cleaning import (clean_text_from_specific_characters, clean_fix_unicode,\n",
    "                                                           clean_remove_currency_symbols, clean_remove_emails,\n",
    "                                                           clean_remove_urls, clean_remove_stopwords)\n",
    "\n",
    "from sem_covid.entrypoints.notebooks.language_modeling.language_model_tools.similarity_calculus import (\n",
    "    euclidean_similarity, manhattan_similarity, cosine_similarity, build_similarity_matrix)\n",
    "\n",
    "from sem_covid.entrypoints.notebooks.language_modeling.language_model_tools.document_handling_tools import (\n",
    "    document_atomization_noun_phrases, lemmatize_document)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "PWDB_TEXTUAL_CLASS = ['title', 'background_info_description', 'content_of_measure_description',\n",
    "                      'use_of_measure_description', 'involvement_of_social_partners_description']\n",
    "\n",
    "DEFAULT_TEXTUAL_COLUMN = ['title']\n",
    "WINDOW = 5\n",
    "MIN_COUNT = 1\n",
    "VECTOR_SIZE = 300\n",
    "EPOCHS = 50\n",
    "EU_TIMELINE_TOTAL_EXAMPLES = 171\n",
    "IRELAND_TIMELINE_TOTAL_EXAMPLES = 410\n",
    "EU_CELLAR_TOTAL_EXAMPLES = 2653\n",
    "\n",
    "KEY_WORDS = ['work', 'agreement', 'working', 'companies', 'workers',\n",
    "             'measures', 'temporary', 'social', 'support', 'covid19',\n",
    "             '2020', 'public', 'national', 'ireland', 'statement', '2021',\n",
    "             'announce', 'health', 'minister', 'new', 'billion', 'coronavirus',\n",
    "             'vaccine', 'eur', 'support', 'million', 'commission', 'eu']\n",
    "\n",
    "COUNTRIES = ['austria', 'belgium', 'bulgaria', 'croatia', 'cyprus', 'czechia', 'denmark', 'estonia',\n",
    "             'european_union', 'finland', 'france', 'germany', 'greece', 'hungary', 'ireland', 'italy',\n",
    "             'latvia', 'lithuania', 'luxembourg', 'malta', 'netherlands', 'norway', 'poland', 'portugal',\n",
    "             'romania', 'slovakia', 'slovenia', 'spain', 'sweden', 'united', 'kingdom']\n",
    "\n",
    "CATEGORY = ['retention', 'workplace', 'labour', 'recovery', 'economic', 'adaptation',\n",
    "            'businesses', 'protection', 'essential', 'workers', 'business_continuity',\n",
    "            'services', 'social', 'market']\n",
    "\n",
    "SUBCATEGORY = ['safety', 'arrangements', 'health', 'spending', 'working', 'support', 'occupational',\n",
    "               'stimulus_packages', 'access', 'time', 'finance', 'remote', 'flexibility', 'workers',\n",
    "               'essential_services', 'remuneration']\n",
    "\n",
    "TARGET_GROUPS_L1 = ['businesses', 'workers', 'citizens']\n",
    "\n",
    "TARGET_GROUPS_L2 = ['company', 'older', 'people', 'female', 'aged', 'corporations', 'businesses',\n",
    "                    'single', 'person', 'larger', 'forms', 'smes', 'ups', 'non', 'single_parents',\n",
    "                    'citizens', 'professions', 'parents', 'groups', 'youth', 'workers', 'essential_services',\n",
    "                    'sector', 'women', 'workplace', 'unemployed', 'care', 'facilities', 'other', 'standard',\n",
    "                    'specific', 'companies', 'self', 'contractors', 'children', 'border', 'solo', 'refugees',\n",
    "                    'minors', 'cross', 'platform', 'employment', 'seasonal', 'disabled', 'migrants', 'start',\n",
    "                    'risk_group', 'commuters', 'employees']\n",
    "\n",
    "FUNDING = ['companies', 'national_funds', 'employer', 'funds', 'national_funds']\n",
    "\n",
    "WORD_GRAPH_CONFIGS = {'category': CATEGORY,\n",
    "                      'subcategory': SUBCATEGORY,\n",
    "                      'key_words': KEY_WORDS,\n",
    "                      'countries': COUNTRIES,\n",
    "                      'target_groups_l1': TARGET_GROUPS_L1,\n",
    "                      'target_groups_l2': TARGET_GROUPS_L2,\n",
    "                      'funding': FUNDING\n",
    "                      }\n",
    "\n",
    "NR1_MODEL_NAME = 'model1'\n",
    "NR2_MODEL_NAME = 'model2'\n",
    "NR3_MODEL_NAME = 'model3'\n",
    "\n",
    "MODEL1_FILE_NAME = 'model1_language_model.model'\n",
    "MODEL2_FILE_NAME = 'model2_language_model.model'\n",
    "MODEL3_FILE_NAME = 'model3_language_model.model'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preprocessing\n",
    "- data cleanup\n",
    "- turn corpus into spacy document"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def add_space_between_dots_and_commas(text: str):\n",
    "    return re.sub(r'(?<=[.,])(?=[^\\s])', r' ', text)\n",
    "\n",
    "\n",
    "def apply_cleaning_functions(document_corpus: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    This function receives the document and leads through cleaning steps\n",
    "    Args:\n",
    "        document_corpus: dataset document corpus\n",
    "\n",
    "    Returns: clean document corpus\n",
    "    \"\"\"\n",
    "    unused_characters = [\"\\\\r\", \">\", \"\\n\", \"\\\\\", \"<\", \"''\", \"%\", \"...\", \"\\'\", '\"', \"(\", \"\\n\", \"*\", \"1)\", \"2)\", \"3)\",\n",
    "                         \"[\", \"]\", \"-\", \"_\", \"\\r\", 'Â®', '..']\n",
    "\n",
    "    new_document_corpus = document_corpus.apply(clean_text_from_specific_characters, characters=unused_characters)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_fix_unicode)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_urls)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_emails)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_currency_symbols)\n",
    "    new_document_corpus = new_document_corpus.apply(clean_remove_stopwords)\n",
    "    new_document_corpus = new_document_corpus.apply(add_space_between_dots_and_commas)\n",
    "\n",
    "    return new_document_corpus\n",
    "\n",
    "\n",
    "def generate_graph(similarity_matrix: pd.DataFrame, graph: nx.Graph, root_word: str,\n",
    "                   top_words: int, threshold: np.float64 = 0.8, deep_level: int = 0,\n",
    "                   max_deep_level: int = 2, deep_map: dict = None, color_map: dict = None) -> nx.Graph:\n",
    "    \"\"\"\n",
    "        Generates d3 graph using the inserted keywords and their top words from similarity matrix\n",
    "    Args:\n",
    "        similarity_matrix: Dataframe with word similarity\n",
    "        graph: networkx graph\n",
    "        root_word: key words\n",
    "        top_words: top similar words from inserted keywords\n",
    "        threshold: minimum percentage of similarity\n",
    "        deep_level: the level of generating leaf\n",
    "        max_deep_level: the maximum number of generated leaf\n",
    "        deep_map: dictionary of the words and their level of similarity\n",
    "        color_map: the color of each level of words' similarity\n",
    "\n",
    "    Returns: a d3 graph with title and root of key word and their similarity words\n",
    "    \"\"\"\n",
    "    if root_word not in deep_map.keys():\n",
    "        deep_map[root_word] = (deep_level, color_map[deep_level])\n",
    "    elif deep_map[root_word][0] > deep_level:\n",
    "        deep_map[root_word] = (deep_level, color_map[deep_level])\n",
    "    if deep_level > max_deep_level:\n",
    "        return graph\n",
    "    new_nodes = similarity_matrix[root_word].sort_values(ascending=False)[:top_words].index.to_list()\n",
    "    new_nodes_weight = list(similarity_matrix[root_word].sort_values(ascending=False)[:top_words].values)\n",
    "    for index in range(0, len(new_nodes)):\n",
    "        if new_nodes_weight[index] >= threshold:\n",
    "            graph.add_edge(root_word, new_nodes[index])\n",
    "            generate_graph(similarity_matrix, graph, new_nodes[index], top_words, threshold, deep_level + 1,\n",
    "                           max_deep_level,\n",
    "                           deep_map=deep_map, color_map=color_map)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def create_graph_for_language_model_key_words(similarity_matrix: pd.DataFrame, language_model_words: list,\n",
    "                                              model_name: str, column_name: str,\n",
    "                                              metric_threshold: np.float64) -> d3graph:\n",
    "    \"\"\"\n",
    "    !!! This is not reusable function. It was made for a single thing !!!\n",
    "\n",
    "    It generates d3graph based on language model selected words and and the similarity\n",
    "    matrix created with those words.\n",
    "    \"\"\"\n",
    "    graph_folder_path = f'docs/word-similarity-web/{model_name}_graphs/{column_name}/'\n",
    "    color_map = {0: '#a70000',\n",
    "                 1: '#f0000',\n",
    "                 2: '#ff7b7b',\n",
    "                 3: '#ffbaba'}\n",
    "    for index in range(0, len(language_model_words)):\n",
    "        deep_map = {}\n",
    "        graph = generate_graph(similarity_matrix, nx.Graph(), language_model_words[index],\n",
    "                               top_words=4, threshold=metric_threshold\n",
    "                               , max_deep_level=2, deep_map=deep_map, color_map=color_map)\n",
    "        network_adjacency_matrix = pd.DataFrame(data=nx.adjacency_matrix(graph).todense(),\n",
    "                                                index=graph.nodes(), columns=graph.nodes())\n",
    "        node_color_list = [deep_map[node][0] for node in graph.nodes()]\n",
    "        d3graph(network_adjacency_matrix, savepath=graph_folder_path, savename=language_model_words[index],\n",
    "                node_color=node_color_list,\n",
    "                width=1920, height=1080, edge_width=5,\n",
    "                edge_distance=60, directed=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class LanguageModelPipeline:\n",
    "    \"\"\"\n",
    "        This pipeline executes the steps for word2vec language training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_sources: List[Tuple[IndexTabularDataSource, List[str]]], language_model_name: str):\n",
    "        \"\"\"\n",
    "            :param dataset_sources: represents the source of the datasets.\n",
    "        \"\"\"\n",
    "        self.dataset_sources = dataset_sources\n",
    "        self.language_model_name = language_model_name\n",
    "        self.documents_corpus = pd.Series()\n",
    "        self.word2vec = None\n",
    "\n",
    "    def download_datasets(self):\n",
    "        \"\"\"\n",
    "            In this step it will download the dataset and detect selected columns.\n",
    "            It can be downloaded as many datasets as there are in data source.\n",
    "        \"\"\"\n",
    "        self.dataset_sources = [(dataset_columns, dataset_source.fetch())\n",
    "                                for dataset_source, dataset_columns in self.dataset_sources]\n",
    "\n",
    "    def extract_textual_data(self):\n",
    "        \"\"\"\n",
    "            After downloading the datasets, the textual data will be found and and concatenated\n",
    "            with executing of several steps as well. It will fill the NaN values with empty space,\n",
    "            add a dot at the end of each concatenated column and reset the index.\n",
    "        \"\"\"\n",
    "        self.documents_corpus = pd.concat([dataset[columns]\n",
    "                                          .fillna(value=\"\")\n",
    "                                          .agg('. '.join, axis=1)\n",
    "                                          .reset_index(drop=True)\n",
    "                                           for columns, dataset in self.dataset_sources\n",
    "                                           ], ignore_index=True)\n",
    "\n",
    "    def clean_textual_data(self):\n",
    "        \"\"\"\n",
    "            The next step is data cleaning. In this step the function \"apply_cleaning_functions\"\n",
    "            applies the following actions:\n",
    "                - clean the document from specific characters\n",
    "                - delete unicode\n",
    "                - removes emails and URLs and currency symbols\n",
    "        \"\"\"\n",
    "        self.documents_corpus = apply_cleaning_functions(self.documents_corpus)\n",
    "\n",
    "    def transform_to_spacy_doc(self):\n",
    "        \"\"\"\n",
    "            When the document is clean, is going to be transform into spacy document\n",
    "        \"\"\"\n",
    "        self.documents_corpus = self.documents_corpus.apply(nlp)\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "            To extract the parts of speech, below it was defined classes for each token is necessary.\n",
    "        \"\"\"\n",
    "        self.documents_corpus = pd.concat([self.documents_corpus,\n",
    "                                           self.documents_corpus.apply(document_atomization_noun_phrases),\n",
    "                                           self.documents_corpus.apply(lemmatize_document)]\n",
    "                                          , ignore_index=True)\n",
    "\n",
    "        self.documents_corpus = self.documents_corpus.apply(lambda x: list(map(str, x)))\n",
    "\n",
    "    def model_training(self):\n",
    "        \"\"\"\n",
    "            When the data is prepared it's stored into Word2Vec model.\n",
    "        \"\"\"\n",
    "        self.word2vec = Word2Vec(sentences=self.documents_corpus, window=WINDOW,\n",
    "                                 min_count=MIN_COUNT, size=VECTOR_SIZE)\n",
    "\n",
    "    def save_language_model(self):\n",
    "        \"\"\"\n",
    "            Saves trained model in MinIO\n",
    "        \"\"\"\n",
    "        minio = store_registry.minio_object_store('mdl-language')\n",
    "        minio.put_object('word2vec/' + self.language_model_name, pickle.dumps(self.word2vec))\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"\n",
    "            The final step is execution, where are stored each step and it will be executed in a row\n",
    "        \"\"\"\n",
    "        self.download_datasets()\n",
    "        self.extract_textual_data()\n",
    "        self.clean_textual_data()\n",
    "        self.transform_to_spacy_doc()\n",
    "        self.extract_features()\n",
    "        self.model_training()\n",
    "        self.save_language_model()\n",
    "\n",
    "\n",
    "class LanguageModelWordsFilter:\n",
    "    def __init__(self, word2vec_model: Word2Vec, key_words: List[str], pos: List[str]) -> None:\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.key_words = key_words\n",
    "        self.pos = pos\n",
    "        self.word2vec_document = None\n",
    "        self.word2vec_document = nlp(' '.join(self.word2vec_model.wv.index2word))\n",
    "        self.word2vec_document = select_pos(self.word2vec_document, self.pos)\n",
    "        self._extract_pos = list(map(str, self.word2vec_document))\n",
    "\n",
    "    def extract_pos(self) -> List[str]:\n",
    "        \"\"\"\n",
    "            transforms a word2vec indexes into spacy document and selects parts of\n",
    "            speech. After that it puts into a list and converts those parts of speech\n",
    "            into strings.\n",
    "        \"\"\"\n",
    "        return self._extract_pos\n",
    "\n",
    "    def select_key_words(self) -> List[str]:\n",
    "        \"\"\"\n",
    "            Finds each word form inserted list of key words and returns a\n",
    "            list with those words if there are presented in the list of\n",
    "            extracted parts of speech.\n",
    "        \"\"\"\n",
    "        return [word for word in self.key_words if word in self._extract_pos]\n",
    "\n",
    "    def select_pos_index(self) -> List[int]:\n",
    "        \"\"\"\n",
    "            Detects the part of speech indexes and returns them into a list\n",
    "        \"\"\"\n",
    "        return [self.word2vec_model.wv.index2word.index(token) for token in self._extract_pos\n",
    "                if token in self.word2vec_model.wv.index2word]\n",
    "\n",
    "    def select_pos_embeddings(self) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "            Detects part of speech embeddings from their indexes\n",
    "        \"\"\"\n",
    "        selected_pos_index = self.select_pos_index()\n",
    "        return [self.word2vec_model.wv.vectors[index] for index in selected_pos_index]\n",
    "\n",
    "\n",
    "def train_language_model(dataset_sources_config: List[tuple], key_words: List[str], language_model_name: str):\n",
    "    \"\"\"\n",
    "        1. creates word2vec model with LanguageModelPipeline\n",
    "        2. filters nouns and adjectives\n",
    "     Args:\n",
    "        dataset_sources_config: the dataset and his textual data columns\n",
    "        language_model_name: the name of the language model, to be saved in MinIO\n",
    "        key_words: the list of keywords\n",
    "    Returns: Amount of filtered words\n",
    "    \"\"\"\n",
    "    model_language_model_pipeline = LanguageModelPipeline(dataset_sources=dataset_sources_config,\n",
    "                                                          language_model_name=language_model_name)\n",
    "    model_language_model_pipeline.execute()\n",
    "\n",
    "    return model_language_model_pipeline.word2vec\n",
    "\n",
    "\n",
    "def filter_language_model_words(language_model: train_language_model, key_words: List[str]):\n",
    "    return LanguageModelWordsFilter(language_model, key_words, pos=['NOUN', 'ADJ'])\n",
    "\n",
    "    # return LanguageModelWordsFilter(model_language_model_pipeline.word2vec, key_words, pos=['NOUN', 'ADJ'])\n",
    "\n",
    "\n",
    "def train_similarity_matrices(model_name: str, language_model: filter_language_model_words):\n",
    "    \"\"\"\n",
    "        Generates similarity matrix using extracts pos and pos embeddings\n",
    "    Args:\n",
    "        model_name: the name of the model that will be saved on the server\n",
    "        language_model: trained word2vec model\n",
    "    \"\"\"\n",
    "    similarity_functions = [cosine_similarity, euclidean_similarity, manhattan_similarity]\n",
    "    for index in range(len(similarity_functions)):\n",
    "        print('Start computing similarity matrix.')\n",
    "        model_similarity_matrix = build_similarity_matrix(language_model.select_pos_embeddings(),\n",
    "                                                          language_model.extract_pos(),\n",
    "                                                          metric=similarity_functions[index])\n",
    "        print('Finish computing similarity matrix.')\n",
    "        print('Save similarity matrix.')\n",
    "        store_registry.minio_object_store('semantic-similarity-matrices').put_object(\n",
    "            f'{model_name}_{similarity_functions[index].__name__}_matrix.json',\n",
    "            model_similarity_matrix.to_json(orient='columns'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class ComplexPipeline:\n",
    "    def __init__(self, language_model_file_name: str, model_name: str):\n",
    "\n",
    "        self.language_model_file_name = language_model_file_name\n",
    "        self.model_name = model_name\n",
    "        self.word2vec = None\n",
    "\n",
    "    def train_language_model(self, dataset_sources_config: List[tuple]):\n",
    "        model_language_model_pipeline = LanguageModelPipeline(dataset_sources=dataset_sources_config,\n",
    "                                                              language_model_name=self.language_model_file_name)\n",
    "        model_language_model_pipeline.execute()\n",
    "        self.word2vec = model_language_model_pipeline.word2vec\n",
    "\n",
    "    def filter_language_model_words(self, key_words: List[str]):\n",
    "        if self.word2vec:\n",
    "            return LanguageModelWordsFilter(self.word2vec, key_words, pos=['NOUN', 'ADJ'])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def train_similarity_matrices(self, key_words: List[str]):\n",
    "        similarity_functions = ['cosine', 'euclidean', 'hamming']\n",
    "        for index in range(len(similarity_functions)):\n",
    "            print('Start computing similarity matrix.')\n",
    "            model_similarity_matrix = build_similarity_matrix(\n",
    "                self.filter_language_model_words(key_words=key_words).select_pos_embeddings(),\n",
    "                self.filter_language_model_words(key_words=key_words).extract_pos(),\n",
    "                metric=similarity_functions[index])\n",
    "            print('Finish computing similarity matrix.')\n",
    "            print('Save similarity matrix.')\n",
    "            store_registry.minio_object_store('semantic-similarity-matrices').put_object(\n",
    "                f'{self.model_name}_{similarity_functions[index]}_matrix.json',\n",
    "                model_similarity_matrix.to_json(orient='columns'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Nr#1 language model based on:\n",
    "- PWDB\n",
    "- eu-timeline\n",
    "- ireland-timeline\n",
    "\n",
    "\n",
    "## Experiment Nr#2 language model based on:\n",
    "- eu-cellar\n",
    "\n",
    "## Experiment Nr#3 language model based on:\n",
    "- PWDB\n",
    "- eu-timeline\n",
    "- ireland-timeline\n",
    "- eu-cellar\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model1_dataset_sources_config = [\n",
    "    (Dataset.PWDB, PWDB_TEXTUAL_CLASS),\n",
    "    (Dataset.EU_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN),\n",
    "    (Dataset.IRELAND_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN)\n",
    "]\n",
    "\n",
    "model2_dataset_sources_config = [\n",
    "    (Dataset.EU_CELLAR_ENRICHED, DEFAULT_TEXTUAL_COLUMN),\n",
    "]\n",
    "\n",
    "model3_dataset_sources_config = [\n",
    "    (Dataset.PWDB, PWDB_TEXTUAL_CLASS),\n",
    "    (Dataset.EU_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN),\n",
    "    (Dataset.EU_CELLAR_ENRICHED, DEFAULT_TEXTUAL_COLUMN),\n",
    "    (Dataset.IRELAND_ACTION_TIMELINE, DEFAULT_TEXTUAL_COLUMN)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train language model and execute similarity matrices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "comp_pipeline = ComplexPipeline(language_model_file_name=MODEL2_FILE_NAME, model_name=NR2_MODEL_NAME)\n",
    "\n",
    "comp_pipeline.train_language_model(model2_dataset_sources_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start computing similarity matrix.\n",
      "Finish computing similarity matrix.\n",
      "Save similarity matrix.\n",
      "Start computing similarity matrix.\n",
      "Finish computing similarity matrix.\n",
      "Save similarity matrix.\n",
      "Start computing similarity matrix.\n",
      "Finish computing similarity matrix.\n",
      "Save similarity matrix.\n",
      "CPU times: user 15 s, sys: 1.23 s, total: 16.2 s\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "comp_pipeline.train_similarity_matrices(key_words=CATEGORY)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "% % time\n",
    "# !!! Below three cells will be executing a good period of time. If for any reason you run it,\n",
    "# take your time, go and make some food, go for a walk maybe. !!!\n",
    "\n",
    "# execution time: 1h 23m\n",
    "model1_word2vec = train_language_model(model1_dataset_sources_config, CATEGORY, MODEL1_FILE_NAME)\n",
    "# model1_similarity_matrix = train_similarity_matrices(NR1_MODEL_NAME, model1_word2vec)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start computing similarity matrix.\n",
      "Finish computing similarity matrix.\n",
      "Save similarity matrix.\n",
      "Create d3Graphs\n",
      "CPU times: user 3min 20s, sys: 787 ms, total: 3min 21s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "# execution time: 3m\n",
    "# model2 = execute_model_steps(NR2_MODEL_NAME, model2_dataset_sources_config)\n",
    "model2_word2vec = train_language_model(model2_dataset_sources_config, MODEL2_FILE_NAME)\n",
    "model2_similarity_matrix = train_similarity_matrices(NR2_MODEL_NAME, model2_word2vec)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "% % time\n",
    "# execution time: 1h 33m\n",
    "model3_word2vec = train_language_model(model3_dataset_sources_config, MODEL3_FILE_NAME)\n",
    "model3_similarity_matrix = train_similarity_matrices(NR3_MODEL_NAME, model3_word2vec)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate D3 Graphs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model1_cosine_matrix = store_registry.minio_object_store('semantic-similarity-matrices').get_object(\n",
    "    'model1_cosine_similarity_matrix.json')\n",
    "\n",
    "model1_language_model = store_registry.minio_object_store('mdl-language').get_object(\n",
    "    'word2vec/model1_language_model.model')\n",
    "\n",
    "model1_graphs = create_graph_for_language_model_key_words(pd.read_json(model1_cosine_matrix),\n",
    "                                                          model1_word2vec.select_key_words(),\n",
    "                                                          model_name=NR1_MODEL_NAME, column_name='category',\n",
    "                                                          metric_threshold=0.4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def plot_graphs(pipeline: ComplexPipeline, model_name: str, model_file_name: str, threshold: np.float64,\n",
    "                word_graph_configs: dict,\n",
    "                normalize_func):\n",
    "    model_cosine_matrix = store_registry.minio_object_store('semantic-similarity-matrices').get_object(model_file_name)\n",
    "    for key in word_graph_configs.keys():\n",
    "        create_graph_for_language_model_key_words(pd.read_json(model_cosine_matrix).applymap(normalize_func),\n",
    "                                                  pipeline.filter_language_model_words(\n",
    "                                                      key_words=word_graph_configs[key]).select_key_words(),\n",
    "                                                  model_name=model_name, metric_threshold=threshold,\n",
    "                                                  column_name=key)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/labour.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/recovery.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/economic.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/adaptation.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/businesses.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/protection.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/essential.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/workers.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/services.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/social.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/market.html\n",
      "[d3graph] >Creating directory [docs/word-similarity-web/model2_graphs/subcategory/]\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/subcategory/safety.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/subcategory/arrangements.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/subcategory/health.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/subcategory/access.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/subcategory/time.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/subcategory/finance.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/subcategory/remote.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/subcategory/flexibility.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/subcategory/workers.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/subcategory/remuneration.html\n",
      "[d3graph] >Creating directory [docs/word-similarity-web/model2_graphs/key_words/]\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/key_words/work.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/key_words/companies.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/key_words/workers.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/key_words/measures.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/key_words/temporary.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/key_words/social.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/key_words/covid19.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/key_words/public.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/key_words/statement.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/key_words/health.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/key_words/new.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/key_words/coronavirus.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/key_words/vaccine.html\n",
      "[d3graph] >Creating directory [docs/word-similarity-web/model2_graphs/countries/]\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/countries/czechia.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/countries/european_union.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/countries/kingdom.html\n",
      "[d3graph] >Creating directory [docs/word-similarity-web/model2_graphs/target_groups_l1/]\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l1/businesses.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l1/workers.html\n",
      "[d3graph] >Creating directory [docs/word-similarity-web/model2_graphs/target_groups_l2/]\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/company.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/people.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/female.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/businesses.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/single.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/person.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/forms.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/non.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/workers.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/sector.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/women.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/other.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/standard.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/specific.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/companies.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/self.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/contractors.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/children.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/border.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/cross.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/employment.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/seasonal.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/start.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/target_groups_l2/employees.html\n",
      "[d3graph] >Creating directory [docs/word-similarity-web/model2_graphs/funding/]\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/funding/companies.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/funding/employer.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/funding/funds.html\n"
     ]
    }
   ],
   "source": [
    "plot_graphs(pipeline = comp_pipeline,\n",
    "            model_name = NR2_MODEL_NAME,\n",
    "            model_file_name= 'model2_cosine_matrix.json',\n",
    "            threshold=0.6,\n",
    "            word_graph_configs=WORD_GRAPH_CONFIGS,\n",
    "            normalize_func= lambda x: 1-x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/labour.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/recovery.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/economic.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/adaptation.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/businesses.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/protection.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/essential.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/workers.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/services.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/social.html\n",
      "Writing /home/jovyan/work/sem-covid/docs/word-similarity-web/model2_graphs/category/market.html\n"
     ]
    }
   ],
   "source": [
    "model2_cosine_matrix = store_registry.minio_object_store('semantic-similarity-matrices').get_object(\n",
    "    'model2_cosine_matrix.json')\n",
    "\n",
    "model2_graphs = create_graph_for_language_model_key_words(pd.read_json(model2_cosine_matrix).applymap(lambda x: 1 - x),\n",
    "                                                          comp_pipeline.filter_language_model_words(\n",
    "                                                              key_words=CATEGORY).select_key_words(),\n",
    "                                                          model_name=NR2_MODEL_NAME, metric_threshold=0.6,\n",
    "                                                          column_name='category')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "temp_df = pd.read_json(model2_cosine_matrix).head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "                   c  parliament  document     union   certain      text  \\\nc           1.000000   -0.088294 -0.027274 -0.210632 -0.097438  0.567301   \nparliament -0.088294    1.000000  0.434603  0.650382  0.181362  0.226026   \ndocument   -0.027274    0.434603  1.000000  0.228784  0.096020  0.186051   \nunion      -0.210632    0.650382  0.228784  1.000000  0.231797  0.009934   \ncertain    -0.097438    0.181362  0.096020  0.231797  1.000000  0.229572   \n\n            economic    social  proposal  communication  ...   bladder  \\\nc           0.092788 -0.017132  0.026077       0.010648  ...  0.349270   \nparliament  0.679419  0.616389  0.742833       0.614692  ...  0.040775   \ndocument    0.578235  0.494430  0.497438       0.808421  ...  0.262805   \nunion       0.584360  0.610301  0.511615       0.420261  ...  0.136103   \ncertain     0.184433  0.264371  0.049539       0.128032  ...  0.647518   \n\n               crise   flavour   sandbox    master    exempt  disruptor  \\\nc           0.345540 -0.054862  0.126926  0.080460 -0.246717  -0.067880   \nparliament  0.064712 -0.009583 -0.183560  0.077561  0.142433  -0.297098   \ndocument    0.290013 -0.333620  0.170464  0.567621 -0.217398   0.120096   \nunion       0.189605  0.151163 -0.015864  0.085663 -0.092072  -0.178300   \ncertain     0.624609  0.469108  0.388410  0.465699  0.413367   0.574125   \n\n              emerge  comprise  ingredient  \nc          -0.227131  0.157701    0.320721  \nparliament -0.053682 -0.051120   -0.005201  \ndocument   -0.366120  0.074215    0.318565  \nunion       0.051018  0.140259   -0.061578  \ncertain     0.616874  0.424554    0.584702  \n\n[5 rows x 3443 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>c</th>\n      <th>parliament</th>\n      <th>document</th>\n      <th>union</th>\n      <th>certain</th>\n      <th>text</th>\n      <th>economic</th>\n      <th>social</th>\n      <th>proposal</th>\n      <th>communication</th>\n      <th>...</th>\n      <th>bladder</th>\n      <th>crise</th>\n      <th>flavour</th>\n      <th>sandbox</th>\n      <th>master</th>\n      <th>exempt</th>\n      <th>disruptor</th>\n      <th>emerge</th>\n      <th>comprise</th>\n      <th>ingredient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>c</th>\n      <td>1.000000</td>\n      <td>-0.088294</td>\n      <td>-0.027274</td>\n      <td>-0.210632</td>\n      <td>-0.097438</td>\n      <td>0.567301</td>\n      <td>0.092788</td>\n      <td>-0.017132</td>\n      <td>0.026077</td>\n      <td>0.010648</td>\n      <td>...</td>\n      <td>0.349270</td>\n      <td>0.345540</td>\n      <td>-0.054862</td>\n      <td>0.126926</td>\n      <td>0.080460</td>\n      <td>-0.246717</td>\n      <td>-0.067880</td>\n      <td>-0.227131</td>\n      <td>0.157701</td>\n      <td>0.320721</td>\n    </tr>\n    <tr>\n      <th>parliament</th>\n      <td>-0.088294</td>\n      <td>1.000000</td>\n      <td>0.434603</td>\n      <td>0.650382</td>\n      <td>0.181362</td>\n      <td>0.226026</td>\n      <td>0.679419</td>\n      <td>0.616389</td>\n      <td>0.742833</td>\n      <td>0.614692</td>\n      <td>...</td>\n      <td>0.040775</td>\n      <td>0.064712</td>\n      <td>-0.009583</td>\n      <td>-0.183560</td>\n      <td>0.077561</td>\n      <td>0.142433</td>\n      <td>-0.297098</td>\n      <td>-0.053682</td>\n      <td>-0.051120</td>\n      <td>-0.005201</td>\n    </tr>\n    <tr>\n      <th>document</th>\n      <td>-0.027274</td>\n      <td>0.434603</td>\n      <td>1.000000</td>\n      <td>0.228784</td>\n      <td>0.096020</td>\n      <td>0.186051</td>\n      <td>0.578235</td>\n      <td>0.494430</td>\n      <td>0.497438</td>\n      <td>0.808421</td>\n      <td>...</td>\n      <td>0.262805</td>\n      <td>0.290013</td>\n      <td>-0.333620</td>\n      <td>0.170464</td>\n      <td>0.567621</td>\n      <td>-0.217398</td>\n      <td>0.120096</td>\n      <td>-0.366120</td>\n      <td>0.074215</td>\n      <td>0.318565</td>\n    </tr>\n    <tr>\n      <th>union</th>\n      <td>-0.210632</td>\n      <td>0.650382</td>\n      <td>0.228784</td>\n      <td>1.000000</td>\n      <td>0.231797</td>\n      <td>0.009934</td>\n      <td>0.584360</td>\n      <td>0.610301</td>\n      <td>0.511615</td>\n      <td>0.420261</td>\n      <td>...</td>\n      <td>0.136103</td>\n      <td>0.189605</td>\n      <td>0.151163</td>\n      <td>-0.015864</td>\n      <td>0.085663</td>\n      <td>-0.092072</td>\n      <td>-0.178300</td>\n      <td>0.051018</td>\n      <td>0.140259</td>\n      <td>-0.061578</td>\n    </tr>\n    <tr>\n      <th>certain</th>\n      <td>-0.097438</td>\n      <td>0.181362</td>\n      <td>0.096020</td>\n      <td>0.231797</td>\n      <td>1.000000</td>\n      <td>0.229572</td>\n      <td>0.184433</td>\n      <td>0.264371</td>\n      <td>0.049539</td>\n      <td>0.128032</td>\n      <td>...</td>\n      <td>0.647518</td>\n      <td>0.624609</td>\n      <td>0.469108</td>\n      <td>0.388410</td>\n      <td>0.465699</td>\n      <td>0.413367</td>\n      <td>0.574125</td>\n      <td>0.616874</td>\n      <td>0.424554</td>\n      <td>0.584702</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã 3443 columns</p>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.applymap(lambda x: 1 - x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model3_cosine_matrix = store_registry.minio_object_store('semantic-similarity-matrices').get_object(\n",
    "    'model3_cosine_similarity_matrix.json')\n",
    "\n",
    "model3_graphs = create_graph_for_language_model_key_words(pd.read_json(model3_cosine_matrix),\n",
    "                                                          model3_word2vec.select_key_words(),\n",
    "                                                          model_name=NR3_MODEL_NAME, metric_threshold=0.4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model2_cosine_matrix = store_registry.minio_object_store('semantic-similarity-matrices').get_object(\n",
    "    'model2_cosine_similarity_matrix.json')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "tmp_df = pd.read_json(model2_cosine_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "tmp_df = tmp_df.where(np.triu(np.ones(tmp_df.shape)).astype(np.bool))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "tmp = tmp_df.stack()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "indexes = tmp.index.to_flat_index()\n",
    "values = tmp.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "data = [indexes[index] + (values[index],) for index in range(0, len(indexes))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "data_updated = [d for d in data if d[2] > 0.4]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "9303141"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "6379797"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_updated)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "[('c', 'c', 1.0),\n ('c', 'case', 0.5449162722),\n ('c', 'text', 0.5692722201),\n ('c', 'relevance', 0.6160866022),\n ('c', 'request', 0.4078985453),\n ('c', 'court', 0.45723506810000003),\n ('c', 't', 0.5243614912),\n ('c', 'p.', 0.6480842233),\n ('c', 'judgment', 0.4635605216),\n ('c', 'final', 0.4800850451),\n ('c', 'raise', 0.6436561942),\n ('c', 'eea_relevance', 0.6934002638),\n ('c', 'concentration', 0.5851275921),\n ('c', 'stability', 0.5182048678),\n ('c', 'i/01', 0.7251908183),\n ('c', 'obligations', 0.5764827728),\n ('c', 'objections', 0.5652606487),\n ('c', 'gmbh', 0.4314999282),\n ('c', 'notification', 0.4941498339),\n ('c', 'authentic', 0.6516040564000001),\n ('c', 'management', 0.42047229410000003),\n ('c', 'p', 0.47767692800000006),\n ('c', 'objection', 0.5742188096),\n ('c', 'president', 0.4273077846),\n ('c', 'ad', 0.6125755310000001),\n ('c', 'candidate', 0.843029201),\n ('c', 'opposition', 0.4285845459),\n ('c', 'e', 0.6431210041000001),\n ('c', 'council_opinion', 0.4901045859),\n ('c', 'group', 0.6348670125),\n ('c', 'convergence', 0.4807579517),\n ('c', 'epso', 0.6142469645),\n ('c', 'capital', 0.5959295034000001),\n ('c', 'a.', 0.5138916969),\n ('c', 'i/02', 0.7106025219000001),\n ('c', 'english', 0.6056940556),\n ('c', 'french', 0.4951648414),\n ('c', 'dusseldorf', 0.40407678480000003),\n ('c', 'es', 0.4240112901),\n ('c', 'airlines', 0.4263698459),\n ('c', 'lawyer', 0.49082461),\n ('c', 'italian', 0.5293591619),\n ('c', 'commission_regulation', 0.4059154391),\n ('c', 'cns', 0.4353219271),\n ('c', 'state_aid_decision_raise_objections', 0.6709957719),\n ('c', 'post', 0.4352549613),\n ('c', 'eighth', 0.5243734717),\n ('c', 'fourth', 0.4163828492),\n ('c', 'german', 0.6743006706),\n ('c', 'du', 0.4224746227),\n ('c', 'business', 0.4512659311),\n ('c', 'non_opposition', 0.5445307493),\n ('c', 'nle', 0.49027881030000003),\n ('c', 'hamburg', 0.41135525700000003),\n ('c', 'recommendation_council_recommendation', 0.5252435803000001),\n ('c', 'simplify', 0.6495054364),\n ('c', 'media', 0.5914919972),\n ('c', 'world', 0.45809474590000004),\n ('c', 'text_eea_relevance', 0.6065117121),\n ('c', '23april', 0.4757799506),\n ('c', 'language', 0.4318296313),\n ('c', 'eurowings', 0.4421241879),\n ('c', 'tenth', 0.4369599223),\n ('c', 'council_decision', 0.4559395909),\n ('c', 'sixth', 0.5057161450000001),\n ('c', 'limited', 0.4553339779),\n ('c', 'case_t', 0.4494797587),\n ('c', 'grand', 0.47390666600000003),\n ('c', 'o.', 0.44289281960000004),\n ('c', '2014/67', 0.5592203736),\n ('c', 'definition', 0.4046148658),\n ('c', 'corporation', 0.6679739356000001),\n ('c', 'distribution', 0.4361151755),\n ('c', 'effects', 0.4069966674),\n ('c', 'sad', 0.41539639230000003),\n ('c', 'sp', 0.4403481483),\n ('c', 'presidency', 0.40158885720000004),\n ('c', 'half', 0.48346325760000003),\n ('c', '15april', 0.5582495332),\n ('c', 'rechtbank', 0.4530233443),\n ('c', 'i/03', 0.7814574242000001),\n ('c', 'way', 0.4332468808),\n ('c', 'sc', 0.6153244376),\n ('c', 'grade', 0.4935311973),\n ('c', 'website', 0.4645236433),\n ('c', 'img', 0.5053194165),\n ('c', 'commission_decision_eu', 0.4924401641),\n ('c', 'clinical', 0.43433287740000004),\n ('c', 'eit', 0.5304621458000001),\n ('c', 'administrators', 0.6155771613000001),\n ('c', 'mr', 0.529268384),\n ('c', 'director', 0.5709738731),\n ('c', 'council_recommendation_eu', 0.4294890463),\n ('c', 'flightright', 0.4690456986),\n ('c', 'deutschland', 0.4599811733),\n ('c', 'modification', 0.42714715000000003),\n ('c', 'dc', 0.5246163011),\n ('c', 'egf/2020/002', 0.49898299570000004),\n ('c', 'airhelp', 0.4604283571),\n ('c', 'polish', 0.4617947638)]"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_updated[:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.        , 0.89442719, 1.        ],\n       [0.89442719, 1.        , 0.89442719],\n       [1.        , 0.89442719, 1.        ]])"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "X = [[1, 1], [2, 6], [1, 1]]\n",
    "Y = [[1, 1], [6, 8], [1, 2]]\n",
    "pairwise_distances(X, metric=cosine_similarity)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "X = np.random.rand(20000, 2)\n",
    "Y = np.random.rand(20000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.94 s, sys: 768 ms, total: 2.71 s\n",
      "Wall time: 2.71 s\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "t = pdist(X, metric='cosine')\n",
    "df = pd.DataFrame(squareform(t), columns=Y, index=Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.97110749, 0.07245815],\n       [0.85690828, 0.32851239],\n       [0.12933652, 0.39534397],\n       ...,\n       [0.30519707, 0.03996792],\n       [0.62630627, 0.10839572],\n       [0.44943443, 0.58265998]])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.83 s, sys: 8.22 ms, total: 3.84 s\n",
      "Wall time: 3.83 s\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "t = squareform(pdist(X, metric=cosine_similarity))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.36 s, sys: 612 ms, total: 2.97 s\n",
      "Wall time: 2.96 s\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "comb = list(combinations(Y, 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.7758772138103938, 0.4581528241795998),\n (0.7758772138103938, 0.7556128188929753),\n (0.7758772138103938, 0.2903824491529986),\n (0.7758772138103938, 0.25190906349060127),\n (0.7758772138103938, 0.09149688546569823),\n (0.7758772138103938, 0.8717632183754248),\n (0.7758772138103938, 0.9347832066775141),\n (0.7758772138103938, 0.4876551931324218),\n (0.7758772138103938, 0.34752915380001737),\n (0.7758772138103938, 0.10621645458722284)]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.73 s, sys: 1.26 s, total: 9.99 s\n",
      "Wall time: 9.99 s\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "indexes = [(Y[i], Y[j]) for i in range(0, len(Y)) for j in range(i + 1, len(Y))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = pdist(X, metric='cosine')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 499 ms, sys: 152 ms, total: 652 ms\n",
      "Wall time: 651 ms\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "#indexes = list(combinations(Y,2))\n",
    "t = pdist(X, metric='cosine')\n",
    "df = pd.DataFrame(squareform(t), columns=Y, index=Y)\n",
    "#data = [indexes[index] + (t[index],) for index in range(0,len(t)) if t[index]>0.4]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.0937090375708306, 0.35141603903249585, 0.7367782219092458),\n (0.0937090375708306, 0.2693672166378879, 0.7253226718686918),\n (0.0937090375708306, 0.6584325242242225, 0.5764058723983649),\n (0.0937090375708306, 0.7465246584985801, 0.7111720596540034),\n (0.0937090375708306, 0.11147079903121071, 0.7160456618923694),\n (0.0937090375708306, 0.5574183124925692, 0.6100590615603838),\n (0.0937090375708306, 0.26776601564358826, 0.5629955186610089),\n (0.0937090375708306, 0.5314425438770268, 0.5032685757247006),\n (0.0937090375708306, 0.8850698657698504, 0.5619252075068168),\n (0.0937090375708306, 0.0931728509723988, 0.5603959774335598),\n (0.0937090375708306, 0.3783660393765643, 0.702514022029491),\n (0.0937090375708306, 0.17301578127648598, 0.46589331284878976),\n (0.0937090375708306, 0.218771250375704, 0.5292387176022288),\n (0.0937090375708306, 0.020460847024664663, 0.43873327218443803),\n (0.0937090375708306, 0.01590531508609827, 0.4097141308133482),\n (0.0937090375708306, 0.8664807937459708, 0.5692380251638701),\n (0.0937090375708306, 0.057024521117212235, 0.5239136732761775),\n (0.0937090375708306, 0.5287695205579951, 0.40891391446590475),\n (0.0937090375708306, 0.31575728836555117, 0.7830854031091173),\n (0.0937090375708306, 0.16606922673902214, 0.7214306116729012),\n (0.0937090375708306, 0.3189169318464363, 0.7836333672246303),\n (0.0937090375708306, 0.818745962291664, 0.6017527760177589),\n (0.0937090375708306, 0.00409065365314365, 0.5047271187151764),\n (0.0937090375708306, 0.8117860945343021, 0.6241739618147824),\n (0.0937090375708306, 0.6743310786697616, 0.4806282524114328),\n (0.0937090375708306, 0.08046034459578832, 0.585271873539021),\n (0.0937090375708306, 0.9489024007874526, 0.6823431399757347),\n (0.0937090375708306, 0.42859124190668685, 0.7460135250444918),\n (0.0937090375708306, 0.22352446798026726, 0.474930765461964),\n (0.0937090375708306, 0.29449385506396675, 0.5771389721453823),\n (0.0937090375708306, 0.8632820442108919, 0.5268021684580353),\n (0.0937090375708306, 0.7221888882972207, 0.4192353376623602),\n (0.0937090375708306, 0.4014055823629321, 0.5011893874352233),\n (0.0937090375708306, 0.14741065427528133, 0.5676843337707426),\n (0.0937090375708306, 0.18269057968934732, 0.40452421373857383),\n (0.0937090375708306, 0.857055112606713, 0.6280084819301499),\n (0.0937090375708306, 0.773362681835847, 0.5198382227405406),\n (0.0937090375708306, 0.7522141347162281, 0.5255588222083774),\n (0.0937090375708306, 0.3860707610475135, 0.6141281762778324),\n (0.0937090375708306, 0.8890459310295509, 0.7259853739497735),\n (0.0937090375708306, 0.7141374128358214, 0.5335769267886197),\n (0.0937090375708306, 0.31878672286948595, 0.7364761236308197),\n (0.0937090375708306, 0.415831005303265, 0.5400492893219522),\n (0.0937090375708306, 0.453471714503242, 0.560660802118636),\n (0.0937090375708306, 0.03275963739694432, 0.48743990544675797),\n (0.0937090375708306, 0.1626795112004239, 0.49085969117625317),\n (0.0937090375708306, 0.21846574091294924, 0.721218568790314),\n (0.0937090375708306, 0.05942875781030099, 0.5118996530462048),\n (0.0937090375708306, 0.27254918908404024, 0.5571640067236061),\n (0.0937090375708306, 0.06395485326258532, 0.6651792807787993),\n (0.0937090375708306, 0.7708808001755167, 0.7188597875465417),\n (0.0937090375708306, 0.8376104029987292, 0.5381280124352863),\n (0.0937090375708306, 0.055617725479988445, 0.48151268203758446),\n (0.0937090375708306, 0.10684619634471793, 0.5717562932158979),\n (0.0937090375708306, 0.012039559985097203, 0.6018128937449803),\n (0.0937090375708306, 0.05715131979524979, 0.7657281920319446),\n (0.0937090375708306, 0.9172914095763505, 0.41508292810566827),\n (0.0937090375708306, 0.4442759506473667, 0.4452225966334572),\n (0.0937090375708306, 0.1895519019530234, 0.7787540181074626),\n (0.0937090375708306, 0.02307750122605501, 0.43711490909153594),\n (0.0937090375708306, 0.9669280010644042, 0.5094931275511162),\n (0.0937090375708306, 0.8717534657788074, 0.510540422510628),\n (0.0937090375708306, 0.8108126967954116, 0.4731522573426433),\n (0.0937090375708306, 0.2556060606860051, 0.7660118971247132),\n (0.0937090375708306, 0.35919271606288783, 0.4979681592479367),\n (0.0937090375708306, 0.8156720165602476, 0.7061856769352179),\n (0.0937090375708306, 0.48780299254799675, 0.5163576130070884),\n (0.0937090375708306, 0.22866990265371867, 0.5213615110905917),\n (0.0937090375708306, 0.14263539501480238, 0.44458767970580715),\n (0.0937090375708306, 0.5653838584385662, 0.4831672318493647),\n (0.0937090375708306, 0.9098483816030506, 0.5163952022154876),\n (0.0937090375708306, 0.6769958999964651, 0.5428020102956815),\n (0.0937090375708306, 0.0263700764188729, 0.5382379058962526),\n (0.0937090375708306, 0.583195357796746, 0.41404399534606084),\n (0.0937090375708306, 0.3089187231053987, 0.4037344725425067),\n (0.0937090375708306, 0.7269338553478538, 0.46926661290460736),\n (0.0937090375708306, 0.32312229700753303, 0.6373780185423114),\n (0.0937090375708306, 0.9687206973361635, 0.43528554278079234),\n (0.0937090375708306, 0.23851207023815202, 0.657357461095561),\n (0.0937090375708306, 0.4704970605977693, 0.5048125157999168),\n (0.0937090375708306, 0.5555148554291072, 0.4780163440189983),\n (0.0937090375708306, 0.10636771999824135, 0.449964309845336),\n (0.0937090375708306, 0.20680607293161102, 0.7393631503440605),\n (0.0937090375708306, 0.15647067501719958, 0.7759471471708317),\n (0.0937090375708306, 0.5941186171213617, 0.718168327289368),\n (0.0937090375708306, 0.7239286713578864, 0.5161493491837081),\n (0.0937090375708306, 0.7129764358738121, 0.5811711426881405),\n (0.0937090375708306, 0.06442323823131824, 0.5022013828223899),\n (0.0937090375708306, 0.4160124622362523, 0.7876326354349875),\n (0.0937090375708306, 0.2915838985811601, 0.4111751276375756),\n (0.0937090375708306, 0.6577407000414475, 0.6801626751696043),\n (0.0937090375708306, 0.6551713300903076, 0.6193250742978362),\n (0.0937090375708306, 0.5939507872017423, 0.5444820067899077),\n (0.0937090375708306, 0.9162716955984788, 0.4895665479620728),\n (0.0937090375708306, 0.5425379455881559, 0.43835742153464075),\n (0.0937090375708306, 0.17697710323846472, 0.5475194658334601),\n (0.0937090375708306, 0.0032778788923957247, 0.582971863590271),\n (0.0937090375708306, 0.13235785080907003, 0.4645665711704796),\n (0.0937090375708306, 0.8557171239069287, 0.7798649829759019),\n (0.0937090375708306, 0.20989356383748692, 0.45108763367771654)]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 878 Âµs, sys: 2.57 ms, total: 3.45 ms\n",
      "Wall time: 2.85 ms\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "t = pdist(X, metric='cosine')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.77 s, sys: 7.99 ms, total: 3.78 s\n",
      "Wall time: 3.78 s\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "t = pairwise_distances(X, metric=cosine_similarity)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 693 ms, sys: 3.97 s, total: 4.66 s\n",
      "Wall time: 552 ms\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "t = pairwise_distances(X, metric='cosine')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.62 s, sys: 7.9 s, total: 10.5 s\n",
      "Wall time: 787 ms\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "t = pairwise_distances(X, metric='cosine', n_jobs=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}